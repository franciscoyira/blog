---
title: A guide to Matching estimators, in R and with DAGs - Part 2
author: Francisco Yirá
date: '2022-03-11'
slug: matching-in-r-part-2
cover: "images/clones_orphan_black.jpg"
useRelativeCover: true
isMath: "true"
categories:
  - causal-inference
  - data-science
  - R
tags:
  - causal-inference-the-mixtape
  - books
  - dags
  - potential-outcomes
  - the-effect
  - matching
  - english-only-ds
---

Welcome to the second part of this article about Matching estimators. If you haven't read the first part, I recommend you to do so, since this second part will build on top of that, but here is a quick summary in case you don't have time for reading another whole article:

The first part covered the conditional independence assumption, which is like a "weak version" of the independence we find in fully randomised trials. It states that the treatment assignment is random inside subgroups of the data with the same values of some covariates X, despite not being random in the data as a whole. For example, you could have an experiment with kids in schools where the treatment is randomised, but different schools have different proportions of treatment assignment. There the school would be our X (a confounder and covariate), and there would be independence between the treatment and (TALK ABOUT potential outcomes).

(TODO: finish introduction)

## Matching vs. Regression: What's the difference?

If you're more familiar with DAGs, you may have realised that, DAG-wise, the contexts in which we can use Matching and Multiple Linear Regression for estimating causal effects are the same. Both require the Conditional Independence Assumption (CIA) to hold, which translates to being able to close all the backdoors by conditioning on a set of observable variables (which we call X).

(insert The Office meme "they're the same picture"))

Put another way, if you need to condition on the covariates X to isolate the causal effect, you could either do Matching based on X or to add X as controls to a regression ($Y = D + X$).

So, how to choose between these two techniques? Let's look into the differences between these two methods to answer that question.

### Regression is parametric, Matching is non-parametric

Regression requires us to assume a functional form that links the the treatment (D) and the covariates (X) with the outcome (Y). We call this kind of model "parametric" because it has parameters that are going to be estimated, which are usually known as "the betas" in the regression context.

On the other hand, Matching provides more freedom by not imposing any functional form on the variables. The only two assumptions it relies on are the CIA (which is required by regression too), and common support (which is necessary to obtain an estimate).

For example, if your regression model is $Y = D + X_1 + X_2$ (where $X_1$ and $X_2$ are covariates and observable confounders), you're imposing a specification that assumes:

-   Independence between the treatment effect and the covariates values.
-   Linear and additive effects of the covariates on the outcome (the "beta" of $X_1$ is the same regardless of the value of $X_2$, and vice-versa)..

These assumptions could be true or not, and if they are not true, then our estimate may be biased due to misspecification.

Meanwhile, if we did Matching using $X_1$ and $X_2$, the estimated counterfactual for each unit would be just the (average) actual outcome of the matched unit(s), so there would be no need to make any functional form assumptions. The relationships between the covariates, the treatment and the outcome could be highly non-linear, and the Matching estimator would still work just fine, as long as common support and the CIA holds[^1].

[^1]: It's worth mentioning that regression could also handle non-linear relationships, but we should have to "declare" such non-linearity in the model specification.

Let's see an example with code:

```{r, message=FALSE, warning=FALSE}
library(tidyverse)

df <- tibble(
  x = runif(1000, -1, 4),
  prob_d = ifelse(x > 0.5 & x < 2.5, 0.1, 0.9),
  d = rbinom(1000, 1, prob_d),
  noise = rnorm(1000, sd = 0.1),
  treat_effect = 1,
  y = sin(x) + d*treat_effect + noise
) %>% 
  mutate(d_factor = factor(d, levels=c(0,1), labels=c("Untreated", "Treated")))

ggplot(df,
       aes(x, y,
           color = d_factor)) +
  geom_point() + 
  labs(color = "Treatment status")
```

```{r}
linear_model1 <- lm(y ~ d + x, data = df)

summary(linear_model1)
```

```{r}
confint(linear_model1)
```

Here `x` is a confounder, because it affects both the outcome and the probability of receiving the treatment, and multiple linear regression can't help us because both relationships are non-linear, so the final estimate is heavily biased (despite including the only confounder as control in the regression). Sad!

Now let's use Matching with the same data. Here we're going to use a Matching estimator implemented by a CRAN R package instead of coding it ourselves[^2]:

[^2]: Which is what we should do most of the time. The R functions of the previous blog post had only pedagogical purposes, and in general is better to leverage implementations of statistical methods available in CRAN packages. Not only are they less likely to have errors, but also they have extra goodies such as standard errors calculation and nice `print` or `summary` methods.

```{r, message=FALSE, warning=FALSE}
library(Matching)
M <- Match(df$y, df$d, df$x, M = 5, Weight = 2, caliper = 0.05)
summary(M)
```

As expected, the Matching estimate is much closer to the true treatment effect (and its confidence interval includes the true value) because it's not relying on false linearity assumptions[^3].

[^3]: The linear regression fan club may object that a dataset like this is very unlikely to be found in the "real world", and that it was literally manufactured with the goal of fooling linear regression (which is true). What's more, linear regression is technically *capable* of handling datasets like this, given that we include the non-linear relationships in the specification (for example, `lm(y ~ d + sin(x), data = df)` returns a treatment effect estimate close to 1). But the point is that we should critically assess if we have good enough reasons to assume a specific functional form between the variables, and realise that if we don't, then a non-parametric approach such as Matching may be a better option.

Here is a illustration that makes it clearer what each of the estimators are doing and why the linear regression estimate is so off[^4]:

[^4]: Yes, KNN regression is not technically the same as matching, but for the effect of this visualisation the KNN algorithm illustrates well what matching is doing.

```{r, message=FALSE, warning=FALSE, fig.height=2.5, out.asp=1}
# Matching 
library(FNN)
knn1 <- knn.reg(
  train = dplyr::select(df, x, d),
  y = df$y,
  test = dplyr::select(df, x, d),
  k = 10,
  algorithm = "brute"
)

df_knn <- df %>% 
  mutate(pred_knn = knn1$pred)

# Regression
df_linear <- df %>% 
  modelr::add_predictions(linear_model1, var = "pred_linear")


# Plots
plot_matching <- 
  ggplot(df_knn) +
  aes(x = x, color = d_factor) +
  geom_point(aes(y = y), alpha = 0.3) +
  geom_line(aes(y = pred_knn), size = 1.5) +
  labs(color = "Treatment status",
       title = "Matching") + theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.x = element_blank()
  )

plot_linear <- 
  ggplot(df_linear) +
  aes(x = x, color = d_factor) +
  geom_point(aes(y = y), alpha = 0.3) +
  geom_line(aes(y = pred_linear), size = 1.5) +
  labs(color = "Treatment status",
       title = "Linear regression")

library(patchwork)

(plot_matching / plot_linear) +
  plot_layout(guides = "collect") + 
  plot_annotation(subtitle = "In each subplot, the estimated ATE is the average distance between the two lines")
    
```

### Common support assumption vs extrapolation

Up to this point, it seems that Matching is winning. Why would we want to assume a specific functional form when we can just... don't?

Sadly, there is no free lunch, and the flexibility of Matching comes at a cost: the common support assumption. In order to get a counterfactual estimate without a parametric model we need to have observations with the same (or similar) covariate values and opposite treatment status in our data. If we don't, Matching will fail.

However, Regression doesn't have such requisite. Instead, it leverages the parametric specification to get estimates by "extrapolation" in the areas without common support. And, if our parametric specification appropriately reflects the data generating process, such extrapolation is an appropriate way to handle these cases[^5].

[^5]: Big If, though.

Let's see an example with simulated data to better understand this idea. The code here will be very similar to the previous example, but with two key differences:

-   Treatment assignment (`d`) is now a *deterministic* function of `x`: for some values of `x` all the units get the treatment, and for other values nobody is treated. This kills the common support assumption, and therefore any chance of using Matching ☠️.

-   However, now we assume that we *know* the non-linear function that links `x` with `y`, and we leverage that knowledge in the regression specification, which now will be `y ~ d + sin(x)`.

Here is the simulated data:

```{r}
df_wo_common_support <-
  tibble(
    x = runif(1000,-1, 4),
    d = ifelse(x > 0.5 & x < 2.5, 0, 1),
    noise = rnorm(1000, sd = 0.1),
    treat_effect = 1,
    y = sin(x) + d * treat_effect + noise
  ) %>%
  mutate(d_factor = factor(
    d,
    levels = c(0, 1),
    labels = c("Untreated", "Treated")
  ))

ggplot(df_wo_common_support,
       aes(x, y,
           color = d_factor)) +
  geom_point() + 
  labs(color = "Treatment status",
       subtitle = "Dataset without common support due to deterministic treatment assignement")
```

And here is the regression with the new, non-linear specification:

```{r}
reg_wo_common_support <- lm(y ~ d + sin(x), data = df_wo_common_support)

summary(reg_wo_common_support)
```

You could argue that this dataset is more challenging than the previous one, but nevertheless regression is able to handle it because it can now extrapolate by using an appropriate specification.

Here is a visualisation of what regression is doing under the hood:

```{r}
# Hacky data manipulation to avoid the plot looking weird
df_wo_common_support <- df_wo_common_support %>%
  mutate(group = case_when(
           x < 0.5 ~ "segment1",
           x > 2.5 ~ "segment3",
           TRUE ~ "segment2"
         ))

# Function to label the treatment variable 
creating_factor_d <- function(x) factor(x, levels = c(0, 1), labels = c("Untreated", "Treated"))

df_wo_cs_treated <- df_wo_common_support %>% 
  mutate(extrapolation = ifelse(d == 1, "No", "Yes"),
         d = 1,
         d_factor = creating_factor_d(d)) %>% 
  modelr::add_predictions(reg_wo_common_support, var = "pred_treated")
  
df_wo_cs_untreated <- df_wo_common_support %>% 
  mutate(extrapolation = ifelse(d == 0, "No", "Yes"),
         d = 0,
         d_factor = creating_factor_d(d)) %>% 
  modelr::add_predictions(reg_wo_common_support, var = "pred_untreated")

plot_wo_cs_reg <- 
  ggplot() +
  aes(x, y, color = d_factor) +
  geom_point(data= df_wo_common_support, alpha = 0.3) +
  geom_line(data = df_wo_cs_untreated,
            aes(y = pred_untreated,
                alpha = extrapolation,
                linetype = extrapolation,
                group = group), size = 1.5) +
  geom_line(data = df_wo_cs_treated,
            aes(y = pred_treated,
                alpha = extrapolation,
                linetype = extrapolation,
                group = group), size = 1.5) +
  scale_alpha_manual(values = c("Yes" = 0.5, "No" = 1)) +
  scale_linetype_manual(values = c("Yes" = "dashed", "No" = "solid")) +
  scale_color_discrete(name = "Treated or Not") +
  theme(legend.key.width=unit(1.5,"cm")) +
  labs(subtitle = str_wrap("Regression extrapolates when there isn't common support, and if the model specification is correct, that may be a good thing"))

plot_wo_cs_reg
```

Nice.

Keep in mind that in the real-world rarely do we have such a perfect knowledge about the correct specification. But if you have it, it's a good idea to leverage it through a parametric method like regression.

Now, just for fun, let's see what matching would do in this dataset. We already know that **it will fail because there isn't common support**, so matches are likely to be *terrible* in the sense that they won't be similar to the original units. But if you want visual proof, here we go:

```{r}
x_values <- df_wo_common_support %>% dplyr::select(x)

knn_wo_cs_treated <- knn.reg(
  train = df_wo_common_support %>% 
    filter(d == 1) %>% 
    dplyr::select(x),
  y = df_wo_common_support %>% 
    filter(d == 1) %>% 
    dplyr::pull(y),
  test = x_values,
  k = 15,
  algorithm = "brute"
)

knn_wo_cs_untreated <- knn.reg(
  train = df_wo_common_support %>% 
    filter(d == 0) %>% 
    dplyr::select(x),
  y = df_wo_common_support %>% 
    filter(d == 0) %>% 
    dplyr::pull(y),
  test = x_values,
  k = 15,
  algorithm = "brute"
)

df_untr_matching_wo_cs <-
  tibble(
    y_pred = knn_wo_cs_untreated$pred,
    x = df_wo_common_support$x,
    d = 0
  ) %>%
  mutate(d_factor = creating_factor_d(d))

df_tr_matching_wo_cs <-
  tibble(
    y_pred = knn_wo_cs_treated$pred,
    x = df_wo_common_support$x,
    d = 1
  ) %>%
  mutate(d_factor = creating_factor_d(d),
         group = case_when(
           x < 0.5 ~ "segment1",
           x > 2.5 ~ "segment3",
           TRUE ~ "segment2"
         ))

plot_matching_wo_cs <- 
  ggplot() +
  aes(x, y, color = d_factor) +
  geom_point(data= df_wo_common_support, alpha = 0.3) +
  geom_line(data = df_untr_matching_wo_cs,
            aes(y = y_pred), size = 1.5) +
  geom_line(data = df_tr_matching_wo_cs,
            aes(y = y_pred, group = group), size = 1.5) +
  labs(color = "Treated or Not",
       subtitle = "Matching just fails when there isn't common support") 

plot_matching_wo_cs
```

The estimated ATE would be the average distance between the two lines (*clearly* a bad estimate).

Ideally, in cases like this, you would like your matching estimation to just throw an error instead of returning a terrible estimate[^6]. I couldn't find a way to make `Matching::Match()` throw an error in this situation, but at least we can set the `caliper` to avoid matches that are too far away[^7].

[^6]: This is a general principle that should be followed in many code-related situations: fail early (so you know ASAP that there is a problem) instead of returning garbage (which will mess up with stuff downstream).

[^7]: This caliper is interpreted as standard deviations by the functions. If you're matching on several variables you could also specify different calipers for each one. For more details check the function documentation.

```{r}
M_wo_cs <- Match(df_wo_common_support$y,
                 df_wo_common_support$d,
                 df_wo_common_support$x,
                 M = 5,
                 # Weight = 2 means we use Mahalanobis distance, weirdly 
                 Weight = 2,
                 caliper = 0.05)
summary(M_wo_cs)
```

Then, our alarms should ring after seeing the number of dropped observations (588 of 604) and the number of matched units (only 16). This should be enough to convince yourself that there is not common support here, even if you can't see it visually[^8].

[^8]: We should still visualise the data whenever we can, but if we were doing matching in more dimensions, the lack of common support would be more difficult to see.

### They handle heterogeneous treatment effects differently

For simplicity sake, all the previous examples have assumed a constant treatment effect over all the units. In real life, however, there are a lot of cases when units or subgroups respond differently to the treatment, which is known as having *heterogeneous treatment effects* (HTE). If you suspect this is the case in your data, you have two options:

-   Option 1: sweep the heterogeneity under the rug, and **just estimate average effects** (ATE or/and ATT). This is OK if the variability of the treatment effects is low, but you are at risk of things like getting a positive estimate when the treatment is actually hurting a small subgroup.

-   Option 2: accept the challenge and try to **estimate treatment effects specific for each subgroup**, also known as CATE (Conditional Average Treatment Effect).

CATE estimation could be a blog post on itself, so we're going to leave it out for now and focus on option 1. Here, Matching and Regression differ in how each they weight the group-specific treatment effects to get the average: Matching does it by **weighting on the covariate frequency**, and Regression does it by **weighting on treatment variance**.

**Covariate frequency** is how many observations there are in each subgroup. The higher the frequency in the subgroup, the higher the weight of its specific treatment effect in the global ATE or ATT. For example, if there are different treatment effects for men and women, and 90% of the data are women, then their treatment effect will have a weight of 90% in the "global" ATE estimated through Matching.

**Treatment variance** is literally the variance of `d`, the treatment assignment, inside each subgroup. This variance is maximised when there is a 50/50 split between treated and controls, and is minimised when all the units have the same treatment status. Thus, if you use regression, the treatment effect of a subgroup with a 50/50 split will have a higher weight on the ATE than the effect of a group with 10/90 split[^9].

[^9]: Of course, none of this matters if the treatment effect are the same for all the units, but if they are different, regression and matching will likely return different estimates, even if there is common support and the regression model is correctly specified.

This may sound unintuitive, but it's true, as we can see in the following code example. As in previous examples, here we have an observable confounder (`gender`) which affects both the outcome `y` and the chance to get the treatment `prob_d`. BUT we also have different treatment effects according to `gender`: people with `gender == "other"` have a treatment effect equal to 3 (when they get the treatment) while everyone else has a treatment effect equal to 1.

```{r}
df_hte <- 
  tibble(
    gender = c(rep("men", 450), rep("women", 450), rep("other", 100)),
    baseline_mean = case_when(
      gender == "men" ~ 10,
      gender == "women" ~ 12,
      gender == "other" ~ 8),
    y0 = rnorm(1000, mean = baseline_mean),
    prob_d = ifelse(gender %in% c("men", "women"), 0.05, 0.5),
    d = rbinom(1000, 1, prob_d),
    treat_effect = ifelse(gender %in% c("men", "women"), 1, 3),
    y = y0 + d*treat_effect
  )
```

```{r}
lm(y ~ d + gender, data = df_hte)
```

```{r}
df_hte_matching <- df_hte %>% 
  fastDummies::dummy_cols("gender")

matrix_gender <- df_hte_matching %>% 
    dplyr::select(starts_with("gender_")) %>% 
    as.matrix()

Match(
  Y = df_hte_matching$y,
  Tr = df_hte_matching$d,
  X = matrix_gender,
  exact = TRUE,
  estimand = "ATE"
) %>% summary()
  
```

Regression gives us an ATE estimate equal to 1.66 while Matching returns an estimate equal to 1.25.

The reason for this is the explanation given above: Regression weights the group-specific ATEs by treatment variance. The subgroup with `gender=="other"` has the highest treatment variance AND ALSO a higher treatment effect (3 vs 1 of the other units), so the final estimate is "pushed upwards" by these observations. Matching, on the other hand, returns a lower estimate because it puts a higher weight on the treatment effects of units with `gender %in% c("men", "women")` which are more frequent in the data.

### Regression has more bias, while Matching has more variance

The last difference has to do with the bias-variance trade-off. We already saw how Matching could have more or less bias and variance depending on the way in which we perform it but, regardless of that, Matching tends to have more variance and less bias than Regression.

The main reason for this is that Matching drops the observations in the "donor pool" that aren't similar enough to the original units, while Regression uses them to fit the model anyway. This reduces the sample variance (due to having higher N) and could increase our statistical power (the chance of rejecting the null of zero effect given that there is a real effect), but leads to bias if the model specification is not appropriate[^10].

[^10]: <https://stats.stackexchange.com/a/50641/249455>

## Propensity Scores

Now we'll move into another relevant Matching topic: the use of **Propensity Scores**, which was introduced by Rubin in the 70s. As the name hints, these are *scores* that measure the *propensity* of receiving the treatment for a given unit, conditional on X, the observable confounders ($P(D=1|X)$).

Something cool about propensity scores is that they help us **avoid the curse of dimensionality**. Instead of dealing with the whole feature-space defined by the covariates X, we "collapse" it in a single variable that contains all the relevant information that explains the treatment assignment. In that sense, propensity scores constitute some sort of **dimensionality reduction**.

The DAG of a propensity score would be something like this:

![*DAG of the propensity score. Source: [Causal Inference - The Mixtape](https://mixtape.scunning.com/matching-and-subclassification.html#propensity-score-methods).*](images/dag_propensity_score.png){width="500"}

As in every DAG, assumptions are given by the existence of arrows and also by the absence of them. In particular, note that **there is no arrow going directly from X to D**. This means we're assuming that all the effect of confounders X on D is "mediated" by the propensity score or, put another way, X can't provide any extra information about D after conditioning on the propensity score.

Therefore, **the propensity score is the only covariate we need to control for** to achieve conditional independence and isolate the causal effect of $D$:

$$
(Y^1, Y^0) \perp D  |  p(X)
$$

This leads to **the balancing property of propensity scores**: the distribution of the covariates X should be the same for units with the same propensity score, no matter their treatment status:

$$
P(X|D=1, p(X)) = P(X|D=0, p(X))
$$

And, good news: this property is testable. We can look at segments of our data with similar propensity score, check if there are significant differences in their covariates, and thus determine if our propensity scores are "good enough".

### We still need common support

The propensity score can free us from the curse of dimensionality but not from the need of common support. For each combination of values of X (the original confounders) there should be a positive probability of both being treated and untreated. This means that **the propensity scores should be strictly between 0 and 1**. What's more, the distributions of propensity scores for the treated and untreated units should overlap.

There are several ways of checking this. The most basic and common (but still advisable) is to look at the histogram or density plot of $p(X)$ by treatment status. Here it's an example of a density plot where common support doesn't hold.

(CHECK THIS PART, AFTER READING CIBT I'M NOT SO SURE ABOUT WHAT I'VE WRITTEN HERE)

![*Source: [https://www.jclinepi.com/article/S0895-4356(05)00224-6/fulltext](https://www.jclinepi.com/article/S0895-4356(05)00224-6/fulltext)*](images/density_plot_no_common_support.jpg){width="550"}

Note that we don't need both distributions to look the same (if that was the case, there would be no need for adjustment) but, in order to estimate the ATT, we *do* need a positive density or weight of the untreated distribution along all the treated group distribution. In figure above, there is a big chunk on the upper section of the treated group distribution where we can't find any untreated units, so the existence of common support is questionable.

Another way of checking for common support (suggested by Dehejia and Wahba, 1999) is to create bins based on the propensity score and check that there are observations of both groups in each bin (or at least in the bins where there are treated units, if we just want to estimate the ATT).

### Estimating the propensity scores

Many of the properties mentioned above (e.g. the balancing property) refer to a theoretical *true propensity score*. In real life we don't have access to those but we have to use *estimated propensity scores* instead.

To estimate the propensity scores we have to fit a model using `d` (the treatment assignment) as the response variable and X (the covariates/confounders) as the predictors. A common model choice for this step is **logistic regression** but we could also use nonparametric machine learning methods such as **gradient boosting** or **random forest**[^11].

[^11]: A question you may have here is what is the point of estimating propensity scores using a parametric method like logistic regression when we could just add the covariates as a controls to a one-step multiple linear regression instead. After all, one of the benefits of matching-based methods was allowing for non-linear relationships in a nonparametric fashion but we're loosing that by using regression for the propensity score estimation.

    I looked for an answer to this and found two key advantages of propensity scores through regression versus the good ol' one-step regression ([source A](https://stats.stackexchange.com/questions/8604/how-are-propensity-scores-different-from-adding-covariates-in-a-regression-and), [source B](https://stats.stackexchange.com/a/3443/249455)).

    The first one is that you get common support checks in the second stage. As we saw before, regression extrapolates when there isn't common support and, what's worse, it does it *quietly*. By estimating propensity scores (even with a parametric logistic regression) we gain the chance to look at the histograms/density plots of the scores and use them as a diagnosis tools for how severe is the covariate imbalance and check if we can go ahead and estimate the ATE or the ATT without extrapolation. In fact, you have the option of usining propensity scores JUST as diagnosis tool and then drop them and go ahead with multiple linear regression.

    Another benefit is that, due to dimensionality reduction, we save degrees of freedom in the second stage (when the scores are used) by using just a single number (the score) instead of several covariates.

ML methods [had been shown to perform better at removing covariate imbalance](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2807890/), especially, of course, in contexts of non-linearity and non-additive relationships, so it may be a good idea to go ahead with them if possible.

There are, however, a couple things to keep in mind when using ML models. The first is to **avoid overfitting**, which is when our model learns from the noise or sampling variance instead of the true patterns in the data. Secondly, **we shouldn't optimise for accurately prediction of the treatment status** but for balancing confounders across treated and untreated groups (which is what the "true" propensity score is supposed to do).

This last point also implies that we shouldn't include in the propensity score model any variables that are not confounders (i.e. that don't affect *both* the treatment status and the outcome) *even* if they improve the precision of the treatment status prediction[^12]. Including such variables is not only unhelpful but it can even be harmful to our causal inference endeavour as it adds noise to the propensity scores (you can see an example of this [here in the book *Causal Inference for the Brave and True*](https://matheusfacure.github.io/python-causality-handbook/11-Propensity-Score.html#common-issues-with-propensity-score)).

[^12]: We're not trying to win a Kaggle competition, but to approximate $p(X)$, which, as it notation says, depends on X and nothing else.

The good news is that there are several packages in R that optimise for covariate balance when estimating the scores. I took a quick look at them and liked a lot one named `twang`. This package is being actively developed by the RAND Corporation, has nice methods to measure covariate balance, and supports `gbm` and `xgboost` models to perform the PS estimation.

Let's see a code example. Here we'll use the data from Lalonde ([1986](https://mixtape.scunning.com/references.html#ref-Lalonde1986)), which is included in the `twang` package and also appears in the book *Causal Inference: The Mixtape*. This dataset compares the covariates and outcomes of the participants in a randomised training program aimed at disadvantaged workers in the mid-70s with those of the general population in the same period.

(maybe talk a bit more about Lalonde? even if it's in a footnote)

Of course, there is confounding in this data due to selection bias, which makes a naive difference of means inappropriate as causal effect estimate. But there is also a big control donor pool (the comparison group comes is 2.3 times the size of the treated group). This gives us hope for estimating the ATT by balancing the covariates through propensity scores.

```{r}
library(twang)
data(lalonde)

ps_lalonde_gbm = ps(
  # This is D ~ X, a model with the treatment as responde and
  # the confounders as predictors
  treat ~ age + educ + black + hispan + nodegree +
    married + re74 + re75,
  data = lalonde,
  n.trees = 10000,
  interaction.depth = 2,
  shrinkage = 0.01,
  estimand = "ATT",
  stop.method = "ks.max",
  n.minobsinnode = 10,
  n.keep = 1,
  n.grid = 25,
  ks.exact = NULL,
  verbose = FALSE
)
```

We use `twang::ps()` for estimating the propensity scores, which uses `gbm` by default. Most of the arguments are related to the `gbm` model. Two key argument are `n.trees`, the maximum number of iteration, and `stop.method`, which specifies the balance measure that will be used to choose the optimal number of iterations. Here we use `"ks.max"`, which is the maximum KS value (a measure of dissimilarity) among all the pairs of treated-untreated covariates distributions *after* adjusting for the estimated propensity scores. The lower the `ks.max`, the higher the balance.

We can use `plot()` on the output of `ps()` to visualise the evolution of the balance measure across the iterations:

```{r}
plot(ps_lalonde_gbm)
```

We can clearly see that the balance is maximised somewhere below the 2000th iteration. This "optimal" iteration is the one used for the "final" propensity scores, which are stored under the name `ps` in the returned object.

```{r}
ps_lalonde_gbm$ps
```

### Inverse Probability of Treatment Weighting

Great! We estimated the propensity scores! But how do we use them? A common sense and actually popular answer would be to do *matching* based on them: match each observation with the unit in the donor pool that has the closest propensity score.

Surprisingly, it turns out that *matching on the propensity score* is a bad idea. A paper from 2018 by King and Nielsen showed several problems with this common approach that may even increase the imbalance between groups.

Okey, so matching is out, but then we go back to the first question: how do we use the propensity scores?! There are several valid strategies, but the more recommended in the reference books I checked was one called **inverse probability of treatment weighting** (IPTW).

The key idea of IPWT, as its name says, is to **weight the observations based on the probability of them having the *opposite* treatment status of what they actually have**. For example, if a unit has a low propensity score (indicating it was unlikely to be treated) but it was actually treated, then it will receive a high weight and vice-versa, a treated unit with high propensity score will have a low weight.

Here the book *Causal Inference for the Brave and True* nicely sums up the reason for this:

> If [a treated individual] has a low probability of treatment, that individual looks like the untreated. However, [it] was treated. This must be interesting. We have a treated that looks like the untreated, so we will give that entity a high weight.

Cool, we have the intuition, now let's take a look at the estimator itself:

```{r}
iptw_ate <- function(data,
                                     outcome,
                                     treatment,
                                     prop_score) {
  
  # Renaming the columns for convenience 
  # (i.e. not using {{ }} afterwards)
  data <- data %>% 
    rename(outcome := {{outcome}},
           treatment := {{treatment}},
           prop_score := {{prop_score}})
  
  # Estimation itself
  data %>% 
    mutate(iptw = outcome * (treatment - prop_score) /
             (prop_score * (1 - prop_score))) %>% 
    pull(iptw) %>% 
    mean()

}
```

(some explanation of the math, what happens when `treatment==1` and when `treatment==0`)

There is also an ATT version of the estimator:

```{r}
iptw_att <- function(data,
                     outcome,
                     treatment,
                     prop_score) {
  
  # Renaming the columns for convenience 
  # (i.e. not using {{ }} afterwards)
  data <- data %>% 
    rename(outcome := {{outcome}},
           treatment := {{treatment}},
           prop_score := {{prop_score}})
  
  # Estimation itself
  n_treated <- data %>% filter(treatment == 1) %>% nrow()
  
  data %>% 
    mutate(iptw = outcome * (treatment - prop_score) /
             (1 - prop_score)) %>% 
    pull(iptw) %>% 
    sum() %>% 
    magrittr::divide_by(n_treated)

}
```

What's different here?

Mathematically, here we use `(1 - prop_score)` in the denominator instead of `(prop_score * (1 - prop_score))` and then we sum and divide by the number of *original* *treated* units (instead of dividing by the number of *total* units).

Intuitively, for the treated `treatment - prop_score` and `1 - prop_score` cancel out, and we end up just with `outcome`. In other words, in the ATT IPTW estimator the treated units are not being weighted, only the control units.

(example with visualisation: using the Lalonde dataset?)

problem of IPTW: sensitivity to extreme PS values (and it's actually impossible to estimate with propensity scores equal to

Possible solutions to what we see above:

-   trimming

-   normalised estimator ✨

### Propensity scores as a diagnosis tool

(should I talk about this?)

## Variance of the estimators

### Matching: don't get creative, use proven formulas and software

### IPW: bootstrap

## Double Robust Estimator

(what it is, and the logic behind it)

(example in code)

## Implementations as R packages

(References and so on)
