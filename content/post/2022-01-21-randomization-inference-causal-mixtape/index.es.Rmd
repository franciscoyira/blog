---
title: 'Randomization Inference: una mejor forma de calcular p-values en experimentos'
author: Francisco Yirá
date: '2022-01-21'
slug: randomization-inference-causal-mixtape
categories:
  - inferencia-causal
  - libros
  - data-science
  - R
  - tutorial
tags:
  - causal-inference-the-mixtape
  - resumenes
  - p-values
---

Bienvenidos a un nuevo artículo de la serie dedicada al libro Causal Inference: The Mixtape. En el artículo anterior vimos una introducción a la notación de Potential Outcomes y lo importante que es esta para expresar conceptos de causalidad, tales como la posibilidad de usar la diferencia de medias en experimentos aleatorizados como estimador insesgado de efectos causales.

Sin embargo, un par de conceptos que estuvieron totalmente ausentes en ese artículo fueron los de test de hipótesis y varianza. Hablamos de que la diferencia simple de medias (SDO por sus siglas en inglés) tenía la misma *esperanza* que el efecto causal promedio (ATE) cuando la asignación del tratamiento (D) era independiente de los outcomes potenciales ($Y_i^1, Y_i^0$).

Pero en el mundo real no observamos la esperanza sino que realizaciones particulares: dos monedas tienen la misma esperanza de arrojar cara o sello al ser lanzadas, pero si lanzamos cada una 5 veces, es muy probable que el número de caras y sellos difiera entre ellas, simplemente debido a la fluctuación aleatoria o varianza propia de este proceso generador de datos.

Dada esa variabilidad en la diferencia de medias, ¿qué tan grande debe ser este valor observado para que estemos seguros de que existe una diferencia *real* entre el grupo tratado y el grupo de control? (es decir, una diferencia en las esperanzas).

Esta pregunta usualmente se responde realizando un test de medias/regresión lineal/ANOVA. Estos métodos son ampliamente enseñados en los cursos universitarios de estadística y econometría, y existe sobre mucho material disponible en internet para aprenderlos, así que este post no se centra en ellos.

De lo que hablaremos hoy será de una metodología alternativa a los tests de hipótesis tradicionales para experimentos aleatorizados. Esta metodología se llama **randomization inference**, y también responde la pregunta de *qué tan distintos deben ser los valores de tratados y controles*, pero de una forma distinta.

## ¿Por qué usar randomization inference en vez de tests de hipótesis tradicionales?

Una pregunta que quizás te estes haciendo en este momento es porque valdría la pena invertir tiempo en aprender y aplicar randomizacion inference siendo que ya contamos con tests de hipótesis tradicionales que son ampliamente usados y conocidos. Yo me estaría preguntando lo mismo si estuviera en tu lugar.

Las razones para aplicar randomizacion inference en nuestros tests de hipótesis pueden resumirse en las siguientes:

1.  Al hacer inferencia causal con datos experimentales, la principal fuente de incertidumbre no es el muestreo desde una población mas grande sino que la asignación aleatoria del tratamiento combinada con la imposibilidad de conocer los contrafactuales. Los metodos tradicionales de test de hipótesis no toman esto en consideración, sino que se enfocan en la incertidumbre de muestreo. **Esto es particularmente problemático si trabajamos con grandes datasets administrativos que literalmente representan "toda la data"** (a.k.a. bIg dAtA) tales como A/B testings, y puede traducirse en que subestimemos considerablemente la incertidumbre en nuestros resultados. Randomization inference aborda estos problemas al tomar en cuenta la incertidumbre proveniente de la asignación del tratamiento, por lo que resulta un procedimiento mas apropiado en estos casos.

2.  También existen ventajas al estar en el extremo opuesto: datos pequeños y/o muy pocas unidades tratadas. En esto casos no resulta muy creíble apelar a las propiedades de "muestras grandes" en las que se basan los tests convencionales. En particular, podemos sufrir de una alta vulnerabilidad a o utileros y observaciones de alto leverage, lo cual se traduce en riesgo de over-rejection de la hipótesis nula. Randomization inference nos ayuda en tal situación al ser una metodología más robusta a outliers y observaciones de alto leverage.

3.  Finalmente, aun si no existe ningún problema particular con los tests tradicionales, randomization inference nos entrega mucha mas libertad respecto de los estimadores o estadísticos a usar. Mientras que al hacer un test de medias habitual estamos restringidos a aquellos estimadores para los cuales se ha podido derivar la varianza, randomization inference nos abre las puertas para usar cualquier estadístico escalar que pueda obtenerse a partir de un dataset. Algunos ejemplos útiles son los estadísticos en base a quintiles (por ejemplo, la mediana), en base a rankings, o el estadístico KS (Kolmogorov-Smirnov) que mide diferencias en las funciones de distribución acumuladas.

Y como bonus, hacer randomization inference es cool. En el mundo de la inferencia causal existe una preferencia estética por las metodologías de "placebo" que simulan tratamientos falsos en los datos reales y chequear que *no* encontremos un efecto relevante de este tratamiento falso, y asi estar mas seguros de que nuestras conclusiones sobre el tratamiento verdadero son validas[^1]. Más abajo veremos que randomization inference consiste en algo muy similar a esto.

[^1]: Algunas de las metodologías de inferencia causal que se apalancan mucho en tests de placebo son regresión discontinua y diferencias en diferencias. En artículos posteriores entraré en mas detalle al respecto.

\[mencionar que no dependemos de la distribución gaussiana y que es una metodología no parametrica, o quizás mas abajo\]

## En qué consiste randomization inference, y cómo aplicarlo en R

Primero que todo, volveré a enfatizar el objetivo y contexto en el cual aplicamos este procedimiento: estamos analizando datos provenientes de un experimento aleatorizado y tenemos

-   Un conjunto de observaciones que fueron asignadas aleatoriamente a grupos de tratamiento y control

-   Una variable de resultados (Y)

Y queremos determinar **si el tratamiento tiene algún efecto sobre dicha variable de resultados**.

Ya que casi siempre existirán diferencias entre el grupo tratado y el grupo de control (simplemente por la variabilidad natural del proceso generador de datos), es necesario realizar algún test de hipótesis para determinar si las diferencias observadas son lo suficientemente grandes como para ser consideradas evidencia de un efecto causal.

Estando en esta situación, podemos aplicar **randomization inference** mediante aplicar los siguientes pasos.

### Paso 1: escoger una hipótesis nula "sharp"

En un test de hipótesis tradicional tenemos una hipótesis nula que afirma algo respecto a un parámetro poblacional, por ejemplo, $\beta_1=0$ donde $\beta_1$ es el coeficiente asociado a la variable de tratamiento $D$.

Lo que esta hipótesis nula plantea es que el efecto *promedio* del tratamiento es 0, pero no dice nada acerca de los efectos del tratamiento para cada unidad $\delta_i$.

En cambio, al aplicar randomization inference utilizamos una hipótesis nula *sharp*, es decir, una nula que afirme algo sobre cada una de las unidades.

La más usada y conocida es la **sharp null de Fischer**:

$$
\delta_i=0
$$

Que nos plantea que el tratamiento tiene efecto cero para todas las unidades analizadas.

Para efectos de simplicidad, en este post se usará la sharp null de Fischer, pero notar que podríamos perfectamente testear hipótesis alternativas tales como $\delta_i=1$, $\delta_i=2$, etc. Incluso podríamos testear hipótesis en las que diferentes grupos o unidades tengan valores de $\delta_i$ distintos. Lo único que se requiere es tener una hipótesis que plantee algo respecto de cada uno de los $\delta_i$.

¿Qué ganamos con usar una nula de este tipo? Pues que nos permite **completar las columnas de Potential Outcomes en nuestro dataset**.

Los datos que observamos normalmente tienen esta estructura:

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(magrittr)
library(causaldata)

ri <- causaldata::ri %>% 
  mutate(id_unit = row_number(),
         y0 = as.numeric(y0),
         y1 = as.numeric(y1))

ri
```

Para todas las unidades observamos $D_i$ e $Y_i$, pero solo uno de los outcomes potenciales ($Y_i^0$ o $Y_i^1$). Apalancándonos en la sharp null, podemos completar las columnas `y0` e `y1`.

```{r}
ri_fischer_null <- ri %>% 
  mutate(y0 = y,
         y1 = y)

ri_fischer_null
```

Como estamos asumiendo que el tratamiento tiene un efecto igual a cero para todas las unidades, ambos outcomes potenciales son iguales al outcome observado.

Por supuesto, no estamos afirmando realmente que esto sea así, simplemente queremos descubrir qué tan improbable es haber observado las diferencias entre grupos que estamos observando bajo el supuesto de esta "sharp null".

### Paso 2: escoger un estadístico de test

Para realizar un test de hipótesis necesitamos no solo una hipótesis nula sino que también un **estadístico**. En un test de hipótesis tradicional, sería necesario que este estadístico cuente con un estimador para su varianza, pero al aplicar randomization inference podemos usar literalmente cualquier estadístico escalar que pueda obtenerse a partir de un vector de asignaciones de tratamiento ($D$) y de un vector de outcomes ($Y$).

Uno de los estadísticos más simples que podemos usar es la **diferencia simple de medias (SDO)** entre ambos grupos.

```{r}
# Funcion que define el estadístico: recibe como input un dataframe con las columnas y (outcome) y d (asignacion de tratamiento), y retorna un valor escalar
sdo <- function(data) {
  data %>%
    summarise(te1 = mean(y[d == 1]),
              te0 = mean(y[d == 0]),
              sdo = te1 - te0) %>%
    pull(sdo)
}

sdo(ri_fischer_null)
```

Sin embargo, nada impide usar otros estadísticos de test (esta libertad es de hecho una de las principales ventajas de aplicar randomization inference).

Algunos ejemplos de estadísticos alternativos que podríamos usar son:

-   **Estadísticos de quantiles**, por ejemplo la diferencia de medianas entre grupos (o cualquier otro percentil). Útiles si tenemos outliers u observaciones de alto leverage.

-   **Estadísticos de ranking**, que convierten la variable de outcomes ($Y$) en valores de ranking (1 para el outcome más bajo, 2 para el segundo más bajo, etc) y luego computan una métrica en base a esos rankings (por ejemplo, diferencia de medias o de medianas).

-   **Estadístico KS (Kolmogorov-Smirnov)**, permite identificar diferencias en las distribuciones de los outcomes mediante medir la distancia máxima entre ambas funciones de distribución acumuladas. Este estadístico nos permitiría, por ejemplo, detectar el efecto de un tratamiento que no cambie la media de la variable de resultados, pero sí su varianza o dispersión.

Para mantener la simplicidad, a continuación se usará el SDO como estadístico de test, pero más abajo mostraré ejemplos de código utilizando otros estadísticos.

### Paso 3: simular distintas asignaciones de tratamiento y obtener la distribución del estadístico

El siguiente paso consiste en obtener la distribución de valores que podría tomar el estadístico del test bajo la "sharp" null.

Para esto:

1.  Realizaremos permutaciones en el vector $D$ usando el mismo proceso aleatorio que se utilizó para obtener las asignaciones de tratamiento "originales" o "verdaderas". Por ejemplo, si para asignar el tratamiento se usó un proceso equivalente a lanzar una moneda al aire para cada observación ($B(n, 0.5)$), entonces cada una de las permutaciones deben provenir del mismo proceso.[^2]

    En nuestro ejemplo, el proceso consiste en sortear 4 asignaciones de tratamiento para un total de 8 observaciones, lo cual produce 70 permutaciones posibles en total.

    ```{r}
    perms <- t(combn(ri_fischer_null$id_unit, 4)) %>% asplit(1) 

    perms_df <- 
      tibble(treated_units = perms) %>% 
      transmute(id_perm = row_number(),
                treated_units = map(treated_units, unlist))

    ri_permuted <- 
      crossing(perms_df, ri_fischer_null) %>% 
      mutate(d = map2_dbl(id_unit, treated_units, ~.x %in% .y))

    ri_permuted
    ```

    ```{r}
    ri_permuted %>% 
      pull(id_perm) %>% 
      n_distinct()
    ```

2.  Para cada permutación, simulamos los valores de $Y$ en base a la hipótesis nula escogida en el Paso 1. **Para la sharp null de Fischer literalmente no hay que hacer nada**, porque plantea que $Y_i=Y_i^0=Y_i^1$, es decir, que el vector de outcomes habría sido el mismo independiente de cual hubiera sido la asignación del tratamiento (pero si nuestra *sharp* null fuera algo como $\delta_i=1$ entonces sí habría que cambiar los valores de $Y$ con cada permutación de $D$).

3.  Calcularemos el valor del estadístico (escogido en el paso anterior) para cada una de estas permutaciones y guardamos su valor.

    ```{r}
    perms_stats <- 
      ri_permuted %>%
      group_by(id_perm) %>%
      summarise(te1 = mean(y[d == 1]),
                te0 = mean(y[d == 0]),
                sdo = te1 - te0)

    perms_stats %>% 
      select(id_perm, sdo)
    ```

4.  Listo! Los valores del estadístico obtenidos en (3) nos indican la distribución de este bajo la *sharp* null.

    ```{r}
    ggplot(perms_stats, aes(sdo)) +
      geom_histogram() +
      labs(x = "Estadístico del test (SDO)")
    ```

[^2]: Si se usó clustering o blocking por grupos al hacer la randomización, también debe usarse al obtener las permutaciones.

**¿Cuántas permutaciones se debe realizar?** Lo ideal sería agotar todas permutaciones posibles para conseguir una representación exacta de la distribución del estadístico de test bajo la *sharp* null. Sin embargo, esto solo es factible en datasets muy pequeños: incluso con un dataset de 2000 observaciones ya se vuelve computacionalmente prohibitivo obtener todas las combinaciones posibles del vector $D$.

La alternativa en ese caso es conformarnos con hacer las permutaciones suficientes para obtener una aproximación razonable a la distribución del estadístico[^3].

[^3]: Como referencia, la función `ri2::conduct_ri`, que es una de las implementaciones de randomization inference que existen R, realiza 1000 simulaciones/permutaciones por defecto.

### Paso 4: comparar el estadístico "real" con la distribución simulada para obtener el p-value

```{r}
perms_ranked <- perms_stats %>%
  select(id_perm, sdo) %>% 
  arrange(desc(abs(sdo))) %>% 
  mutate(rank = row_number(desc(abs(sdo)))) %>% 
  group_by(abs(sdo)) %>% 
  mutate(new_rank = max(rank))

perms_ranked
```

```{r}
n <- nrow(perms_ranked)

p_value <- 
  perms_ranked %>% 
  # The first permutation is the one with the true D vector
  filter(id_perm == 1) %>%
  # We get the proportion of permutations above our "true" permutation. That will be our p-value
  pull(new_rank)/n

p_value
```

## Usando el paquete `ri2`

Partir replicando lo mismo que antes (llegar al mismo p-value) \`

```{r}
library(ri2)
```

"In order to conduct randomization inference, we need to supply 1) a test statistic, 2) a null hypothesis, and 3) a randomization procedure."

El procedimiento de randomizacion: tiene que corresponder con como se asigno el tratamiento en primer lugar. Funcion `declare_ra` permite declarar este procedimiento.

```{r}
declaration <- declare_ra(N = 8, m = 4)
declaration
```

<https://stackoverflow.com/questions/19544254/permutation-test-in-r-and-seeing-which-assignments-lead-to-greater-outcome/48628402#48628402>

Para suministrar el estadistico del test, podemos usar el argumento `formula` de la funcion `conduct_ri`. Tambien se puede usar argumento `test_function`: debe ser funcion que recibe un data frame y retorna un escalar (el valor del test).

`sharp_hypothesis`: valor del treatment effect bajo la sharp null. Admite un numero escalar.

```{r}
# Declarando la `test_function`
sdo <- function(data) {
  data %>% 
  summarise(te1 = mean(y[d == 1], na.rm=TRUE),
            te0 = mean(y[d == 0], na.rm=TRUE),
            sdo = te1 - te0) %>% 
    pull(sdo) %>% 
    abs()
}
```

```{r}
ri2_out <- conduct_ri(
  test_function = sdo,
  assignment = "d",
  outcome = "y",
  declaration = declaration,
  sharp_hypothesis = 0,
  data = ri
)

summary(ri2_out)
```

```{r}
# Ambos metodos han generado el mismo set de estadisticos
all(ri2_out$sims_df$est_sim %in% perms_ranked$sdo)
```

```{r}
all.equal(ri2_out$sims_df$est_sim %>% table(),
          perms_stats$sdo %>% table())
# OK, con esto me consta que el conjunto de permutaciones obtenidos son los mismos
# Pero porque hay una diferencia con los p-values!!!!???
```

```{r}
perms_stats
```

```{r}
ri2_out <- conduct_ri(
  formula = y ~ d,
  assignment = "d",
  outcome = "y",
  declaration = declaration,
  sharp_hypothesis = 0,
  data = ri,
  p = "upper",
  sims = 70
)

summary(ri2_out)
```

```{r}
plot(ri2_out)
```

CHALLENGE: Como utilizar el estadistico KS usando test_function?

```{r}
# # Declaramos que tratamiento se asigna a 2 unidades de 7
# declaration <- declare_ra(N = 7, m = 2)
# 
# # Conduct Randomization Inference
# ri2_out <- conduct_ri(
#   # Estadistico del test: diferencia de medias
#   formula = Y ~ Z,
#   declaration = declaration,
#   # Sharp null tradicional
#   sharp_hypothesis = 0,
#   # Dataset con los D e Y observados
#   data = table_2_2
# )
# 
# summary(ri2_out)
```

```{r}
# plot(ri2_out)
```

"A major benefit of randomization inference is we can specify any scalar test statistic, which means we can conduct hypothesis tests for estimators beyond the narrow set for which statisticians have derived the variance."

"Randomization inference is a useful tool because we can conduct hypothesis tests without making additional assumptions about the distributions of outcomes or estimators. We can also do tests for arbitrary test statistics -- we're not just restricted to the set for which statisticians have worked out analytic hypothesis testing procedures."

<https://cran.r-project.org/web/packages/ri2/vignettes/ri2_vignette.html>
