---
title: 'Randomization Inference en R: una mejor forma de calcular p-values en experimentos aleatorios'
author: Francisco Yir√°
date: '2022-01-18'
slug: randomization-inference-causal-mixtape
cover: "images/dices.jpg"
useRelativeCover: true
coverCaption: "Cr√©ditos de la imagen: ¬© Bert Brus | [www.bertbrus.nl](https://www.bertbrus.nl/)"
isMath: "true"
categories:
  - inferencia-causal
  - data-science
  - R
  - tutorial
tags:
  - causal-inference-the-mixtape
  - randomization-inference
  - resumenes
  - p-values
  - libros 
---

Bienvenidos a un nuevo art√≠culo de [la serie](https://www.franciscoyira.com/es/tags/causal-inference-the-mixtape/) dedicada al libro [**Causal Inference: The Mixtape**](https://mixtape.scunning.com/). En el art√≠culo anterior vimos una introducci√≥n a [la notaci√≥n de outcomes potenciales](https://www.franciscoyira.com/es/post/potential-outcomes-causal-inference-mixtape/) y c√≥mo esta permite expresar conceptos claves de inferencia causal.

Uno de esos conceptos claves es que la diferencia de medias simple, en presencia de un tratamiento asignado aleatoriamente, constituye un estimador insesgado del efecto causal del tratamiento (i.e. en esos casos *correlaci√≥n **s√≠** es causalidad*).

Sin embargo, el insesgamiento (que la esperanza sea igual al efecto causal promedio) no es la √∫nica propiedad relevante de un estimador causal. Tambi√©n nos interesa la **varianza**, ya que en el mundo real observamos *realizaciones particulares* del proceso generador de datos que bien podr√≠an estar muy desviadas de la esperanza[^1].

[^1]: Por ejemplo, dos monedas tienen la misma esperanza de arrojar cara o sello al ser lanzadas, pero si lanzamos cada una 5 veces, es muy probable que el n√∫mero de caras y sellos difiera entre ellas, simplemente debido a la fluctuaci√≥n aleatoria o varianza propia de este proceso generador de datos.

Para incorporar esta dimensi√≥n contamos con herramientas como los **tests de medias, la regresi√≥n lineal, y el ANOVA**, que relacionan la diferencia de medias observada con una varianza estimada para indicarnos qu√© tan seguros podemos estar de que exista una diferencia real entre las unidades tratadas y las de control (es decir, de que los procesos generadores de datos sean efectivamente distintos).

Estos m√©todos son muy populares y su uso est√° muy extendido, pero eso no significa que siempre sean la herramienta m√°s apropiada para hacer inferencia causal con datos experimentales. Por ello, hoy hablaremos de una metodolog√≠a alternativa que, en ciertas circunstancias, puede ser mejor opci√≥n para realizar tests de hip√≥tesis sobre experimentos aleatorizados. Su nombre es **Randomization Inference**.

## ¬øPor qu√© molestarse en aprender Randomization Inference?

Una pregunta que quiz√°s te est√©s haciendo a estas alturas es porque valdr√≠a la pena invertir tiempo en aprender y aplicar esta metodolog√≠a siendo que ya contamos con los tests de hip√≥tesis tradicionales que son ampliamente usados y conocidos. Yo me estar√≠a preguntando lo mismo.

![](images/but-why-should-i-care-01.jpg){width="300"}

Las razones para realizar tests de hip√≥tesis basados en Randomization Inference (RI) pueden resumirse en las siguientes:

1.  Al hacer inferencia causal con datos experimentales, la principal fuente de incertidumbre no es el muestreo desde una poblaci√≥n mas grande sino que la asignaci√≥n aleatoria del tratamiento combinada con la imposibilidad de conocer los contrafactuales. Los m√©todos tradicionales de test de hip√≥tesis no toman esto en consideraci√≥n, sino que se enfocan en la incertidumbre de muestreo. **Esto es particularmente problem√°tico si trabajamos con grandes datasets administrativos que literalmente representan "toda la data"** (a.k.a. bIg dAtA), y puede traducirse en que subestimemos considerablemente la incertidumbre en nuestros resultados. RI aborda estos problemas al tomar en cuenta la incertidumbre proveniente de la asignaci√≥n del tratamiento, por lo que es un procedimiento mas apropiado de usar en estos casos.

2.  Tambi√©n existen ventajas al estar en el extremo opuesto: **datos peque√±os y/o con muy pocas unidades tratadas**. En estos casos no resulta muy cre√≠ble apelar a las propiedades de "muestras grandes" en las que se basan los tests convencionales. En particular, podr√≠amos sufrir de una alta vulnerabilidad a *outliers* y observaciones de alto leverage, lo cual se traduce en riesgo de *over-rejection* de la hip√≥tesis nula. RI nos ayuda en tal situaci√≥n al ser **una metodolog√≠a m√°s robusta a *outliers* y observaciones de alto leverage**, en especial cuando se combina con estad√≠sticos de ranking o de cuantiles (los cuales se explicar√°n en detalle m√°s abajo).

3.  Finalmente, a√∫n si no existe ning√∫n problema particular con los tests tradicionales, RI nos entrega mucha m√°s libertad respecto de los estimadores o estad√≠sticos a usar. Mientras que al hacer un test de hip√≥tesis est√°ndar estamos restringidos a aquellos estimadores para los cuales se ha podido estimar la varianza o se ha construido el test de forma anal√≠tica, RI nos abre las puertas para **usar cualquier estad√≠stico escalar que pueda obtenerse a partir de un dataset**, sin siquiera obligarnos a asumir una funci√≥n de distribuci√≥n para el estimador. Algunos ejemplos de estad√≠sticos √∫tiles que podemos usar son los estad√≠sticos de cuantiles (por ejemplo, la mediana), los estad√≠sticos en base a rankings, o el [estad√≠stico KS (Kolmogorov-Smirnov)](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) que mide diferencias en las distribuciones de los *outcomes*.

Y como bonus, **hacer Randomization Inference es cool**, en el sentido de que en el mundo de la inferencia causal existe una preferencia est√©tica por las **metodolog√≠as de "placebo"** y RI puede considerarse como una de ellas. Este tipo de metodolog√≠as se caracterizan por simular tratamientos falsos en datos reales y chequear que *no* encontremos un efecto relevante asociado a esos tratamientos falsos, para as√≠ estar mas seguros de que nuestras conclusiones sobre el tratamiento verdadero son v√°lidas[^2]. M√°s abajo veremos que Randomization Inference consiste en algo muy similar a esto.

[^2]: Algunas de las metodolog√≠as de inferencia causal que se apalancan mucho en tests de placebo son regresi√≥n discontinua y diferencias en diferencias. En art√≠culos posteriores se las discutir√° en m√°s detalle.

## Randomization Inference paso a paso en R

Para pasar a explicar la metodolog√≠a en s√≠, recordemos primero el objetivo y contexto en el cual aplicar√≠amos este procedimiento. Estamos analizando datos provenientes de un experimento aleatorizado y tenemos:

-   Un conjunto de observaciones que fueron asignadas aleatoriamente a grupos de tratamiento y control.

-   Una variable de resultados ($Y$).

Y queremos determinar **si el tratamiento tiene alg√∫n efecto sobre dicha variable de resultados**.

Ya que casi siempre existir√°n diferencias entre el grupo tratado y el grupo de control (simplemente por la variabilidad natural del proceso generador de datos), es necesario realizar alg√∫n test de hip√≥tesis para determinar si las diferencias observadas son lo suficientemente grandes como para ser consideradas evidencia de un efecto causal.

Estando en esta situaci√≥n, podemos realizar un test de hip√≥teis con **Randomization Inference** mediante aplicar los siguientes pasos.

### Paso 1: escoger una hip√≥tesis nula "sharp"

En un test de hip√≥tesis est√°ndar tenemos una hip√≥tesis nula que afirma algo respecto a un par√°metro poblacional, por ejemplo, $\beta_1=0$ donde $\beta_1$ es el coeficiente asociado a la variable de tratamiento $D$. Lo que esta hip√≥tesis nula plantea es que el efecto *promedio* del tratamiento es 0, pero no dice nada acerca de los efectos del tratamiento para cada unidad ($\delta_i$).

En cambio, al aplicar RI utilizamos una **hip√≥tesis nula *sharp***, es decir, una nula que afirma algo sobre los efectos en cada una de las unidades.

La m√°s usada y conocida es la ***sharp*** **null de Fischer**:

$$
\delta_i=0
$$

Que nos plantea que el tratamiento tiene efecto cero para todas las unidades analizadas.

Para efectos de simplicidad, en este post se usar√° la sharp null de Fischer, pero notar que podr√≠amos perfectamente testear hip√≥tesis alternativas tales como $\delta_i=1$, $\delta_i=2$, etc. Incluso podr√≠amos testear hip√≥tesis en las que diferentes grupos o unidades tengan valores de $\delta_i$ distintos. Lo √∫nico que se requiere es tener una hip√≥tesis que plantee algo respecto de cada uno de los $\delta_i$.

¬øQu√© ganamos con usar una nula de este tipo? Pues que nos permite **completar las columnas de Potential Outcomes en nuestro dataset**.

Los datos que observamos normalmente tienen esta estructura:

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(magrittr)
library(causaldata)

ri <- causaldata::ri %>% 
  mutate(id_unit = row_number(),
         y0 = as.numeric(y0),
         y1 = as.numeric(y1))

ri
```

Para todas las unidades observamos $D_i$ e $Y_i$, pero solo uno de los outcomes potenciales ($Y_i^0$ o $Y_i^1$). Apalanc√°ndonos en la *sharp* null, podemos completar las columnas `y0` e `y1` en base a $Y_i=Y_i^0=Y_i^1$

```{r}
ri_fischer_null <- ri %>% 
  mutate(y0 = y,
         y1 = y)

ri_fischer_null
```

Como estamos asumiendo que el tratamiento tiene un efecto igual a cero para todas las unidades, **ambos outcomes potenciales son iguales al outcome observado**.

Por supuesto, no estamos afirmando que esto sea as√≠ realmente, simplemente queremos descubrir qu√© tan improbable es haber observado las diferencias entre grupos que estamos observando bajo el supuesto de esta *sharp* null.

### Paso 2: escoger un estad√≠stico de test

Para realizar un test de hip√≥tesis necesitamos no solo una hip√≥tesis nula sino que tambi√©n un **estad√≠stico**. En un test de hip√≥tesis tradicional, ser√≠a necesario que este estad√≠stico cuente con un estimador para su varianza, pero al aplicar randomization inference podemos usar literalmente cualquier estad√≠stico escalar que pueda obtenerse a partir de un vector de asignaciones de tratamiento ($D$) y de un vector de outcomes ($Y$).

Uno de los estad√≠sticos m√°s simples que podemos usar es la **diferencia simple de medias (SDO)** entre ambos grupos.

```{r}
# Funcion que define el estad√≠stico: recibe como input un dataframe con 
# las columnas `y` (outcome) y `d` (asignacion de tratamiento), y retorna 
# un valor escalar
sdo <- function(data) {
  data %>%
    summarise(te1 = mean(y[d == 1]),
              te0 = mean(y[d == 0]),
              sdo = te1 - te0) %>%
    pull(sdo)
}

sdo(ri_fischer_null)
```

Sin embargo, nada impide usar otros estad√≠sticos de test. Esta libertad es de hecho una de las principales ventajas de aplicar Randomization Inference.

Algunos ejemplos de estad√≠sticos alternativos que podr√≠amos usar son:

-   **Estad√≠sticos de quantiles**, por ejemplo la diferencia de medianas entre grupos (o cualquier otro percentil). √ötiles si tenemos outliers u observaciones de alto leverage.

-   **Estad√≠sticos de ranking**, que convierten la variable de outcomes ($Y$) en valores de ranking (1 para el outcome m√°s bajo, 2 para el segundo m√°s bajo, etc) y luego computan una m√©trica en base a esos rankings (por ejemplo, diferencia de medias o de medianas).

-   **Estad√≠stico KS (Kolmogorov-Smirnov)**, que permite identificar diferencias en las distribuciones de outcomes mediante medir la distancia m√°xima entre ambas funciones de distribuci√≥n acumuladas. Este estad√≠stico nos permitir√≠a, por ejemplo, detectar el efecto de un tratamiento que no afecte la media de $Y$, pero s√≠ su varianza o dispersi√≥n.

Para mantener la simplicidad, a continuaci√≥n se usar√° el SDO como estad√≠stico de test, pero m√°s abajo mostrar√© ejemplos de c√≥digo utilizando el estad√≠stico KS.

### Paso 3: simular distintas asignaciones de tratamiento y obtener la distribuci√≥n del estad√≠stico

El siguiente consiste en obtener la distribuci√≥n de valores que podr√≠a tomar el estad√≠stico del test bajo la *sharp* null.

Para esto:

1.  Realizaremos permutaciones en el vector $D$ mediante el mismo proceso aleatorio que se utiliz√≥ para obtener las asignaciones de tratamiento originales en primer lugar. Por ejemplo, si para asignar el tratamiento se us√≥ un proceso equivalente a lanzar una moneda al aire para cada observaci√≥n ([$B(n, 0.5)$](https://en.wikipedia.org/wiki/Binomial_distribution)), entonces cada una de las permutaciones deben generarse con el mismo proceso.[^3]

    En nuestro ejemplo, el proceso consiste en sortear 4 asignaciones de tratamiento para un total de 8 observaciones, lo cual produce 70 permutaciones posibles en total. El siguiente c√≥digo obtiene tales permutaciones:

    ```{r}
    perms <- t(combn(ri_fischer_null$id_unit, 4)) %>% asplit(1) 

    perms_df <- 
      tibble(treated_units = perms) %>% 
      transmute(id_perm = row_number(),
                treated_units = map(treated_units, unlist))

    ri_permuted <- 
      crossing(perms_df, ri_fischer_null) %>% 
      mutate(d = map2_dbl(id_unit, treated_units, ~.x %in% .y))

    ri_permuted
    ```

    ```{r}
    ri_permuted %>% 
      pull(id_perm) %>% 
      n_distinct()
    ```

2.  Para cada permutaci√≥n, simulamos los valores de $Y$ en base a la hip√≥tesis nula escogida previamente. **Para la sharp null de Fischer literalmente no hay que hacer nada**, ya que al plantear esta que $Y_i=Y_i^0=Y_i^1$, el vector $Y$ se mantiene constante en cada permutaci√≥n de $D$ (sin embargo, si nuestra *sharp* null fuera algo como $\delta_i=1$ entonces s√≠ habr√≠a que cambiar los valores de $Y$ con cada permutaci√≥n).

3.  Calcularemos el valor del estad√≠stico del test (el SDO en nuestro caso) para cada una de estas permutaciones y guardamos su valor.

    ```{r}
    perms_stats <- 
      ri_permuted %>%
      group_by(id_perm) %>%
      summarise(te1 = mean(y[d == 1]),
                te0 = mean(y[d == 0]),
                sdo = te1 - te0)

    perms_stats %>% 
      select(id_perm, sdo)
    ```

4.  Listo! Los valores del estad√≠stico obtenidos en (3) nos indican la distribuci√≥n de este bajo la *sharp* null.

    ```{r, message=FALSE}
    ggplot(perms_stats, aes(sdo)) +
      geom_histogram() +
      labs(x = "Estad√≠stico del test (SDO)")
    ```

[^3]: O si se us√≥ clustering o blocking por grupos al hacer la randomizaci√≥n, tambi√©n debe usarse al obtener las permutaciones.

**¬øCu√°ntas permutaciones se deben realizar?** Lo ideal ser√≠a agotar todas permutaciones posibles para conseguir una representaci√≥n exacta de la distribuci√≥n del estad√≠stico de test bajo la *sharp* null. Sin embargo, esto solo es factible en datasets muy peque√±os: incluso con un dataset de 2000 observaciones ya se vuelve computacionalmente prohibitivo obtener todas las combinaciones posibles del vector $D$.

La alternativa en ese caso es conformarnos con hacer las permutaciones suficientes para obtener una aproximaci√≥n razonable a la distribuci√≥n del estad√≠stico[^4].

[^4]: Como referencia, la funci√≥n `ri2::conduct_ri`, que es una de las implementaciones de Randomization Inference que existen R, realiza 1000 simulaciones/permutaciones por defecto.

### Paso 4: comparar el estad√≠stico "real" con la distribuci√≥n simulada para obtener el p-value

Luego de haber obtenido la distribuci√≥n (exacta o aproximada) del estad√≠stico bajo la *sharp* null, procedemos a ubicar el estad√≠stico "verdadero" (aquel correspondiente los valores reales de $D$ e $Y$) dentro de esa distribuci√≥n.

*Notar que si queremos realizar un test de dos colas (lo m√°s usual) debemos aplicar valor absoluto a la distribuci√≥n de estad√≠sticos (que es de hecho lo que haremos en este ejemplo).*

```{r, message=FALSE}
true_statistic <- perms_stats %>% 
  filter(id_perm == 1) %>% 
  pull(sdo)

ggplot(perms_stats, 
       # Aplicamos valor absoluto porque haremos un test de 2 colas
       aes(abs(sdo))) +
  geom_histogram() +
  labs(x = "Valor absoluto del estad√≠stico del test (SDO)",
       y = "count") +
  geom_vline(xintercept = true_statistic, colour = "red", size = 2) +
  annotate(geom = "label",
           x = true_statistic,
           y = 10,
           label = "Estad√≠stico 'verdadero'",
           colour = "red") +
  annotate("segment", x = true_statistic+1, xend = true_statistic+3, y = 9, yend =9,
           colour = "purple", size = 2, arrow = arrow()) +
  annotate(geom = "label",
           x = true_statistic+2,
           y = 10,
           label = "Estad√≠sticos m√°s extremos",
           colour = "purple")
```

Habiendo hecho esto procedemos a calcular el **p-value** (que es lo que hemos estado buscando desde un comienzo).

Este p-value se calcula en dos pasos:

1.  Se crea un **ranking descendiente de los estad√≠sticos de test** obtenidos en las permutaciones (el con mayor valor tiene ranking 1 y el con menor valor tiene ranking N). Para los estad√≠sticos "empatados" en un mismo valor, su ranking corresponde el m√°ximo de los rankings originales de los estad√≠sticos que comparten tal valor (por ejemplo, si tres estad√≠sticos son iguales y tienen rankings 2, 3, y 4, todos ellos quedan con ranking igual a 4).

```{r}
perms_ranked <- perms_stats %>%
  # Igual que antes, se aplica valor absoluto para tener un test de dos colas
  mutate(abs_sdo = abs(sdo)) %>% 
  select(id_perm, abs_sdo) %>% 
  arrange(desc(abs_sdo)) %>% 
  mutate(rank = row_number(desc(abs_sdo))) %>% 
  group_by(abs_sdo) %>% 
  mutate(new_rank = max(rank))

perms_ranked
```

2.  Se calcula el ratio entre el ranking del estad√≠stico verdadero y el total de permutaciones obtenidas (N).

```{r}
n <- nrow(perms_ranked)

p_value <- 
  perms_ranked %>% 
  filter(id_perm == 1) %>%
  pull(new_rank)/n

p_value
```

En nuestro caso, el p-value obtenido es de 0.86, lo que implica que no podemos rechazar la *sharp* null con un 5% de significancia (ni con ning√∫n nivel de significancia convencional üòÖ). Dicho de otra forma, **el valor de SDO observado no es un valor improbable de observar si asumimos que el tratamiento no tuvo efecto en ninguna unidad** y que la √∫nica fuente de variabilidad en el estad√≠stico es la asignaci√≥n aleatoria de $D$.

## Haci√©ndolo m√°s f√°cil con el paquete *ri2* ü§ó

El c√≥digo en R mostrado arriba ten√≠a como fin explicar en qu√© consiste cada paso del procedimiento. Sin embargo, no es muy conciso y adaptarlo para otros datasets puede resultar un poco tedioso. Por ello, para aplicar Randomization Inference en nuestros propios datos es recomendable usar paquetes de R dise√±ados para este fin, tales como `ri2`.

```{r, message=FALSE}
library(ri2)
```

Algo que no cambia respecto al c√≥digo "manual" es que siempre debemos suministrar 1) un estad√≠stico de test, 2) una hip√≥tesis nula *sharp*, y 3) un procedimiento de randomizaci√≥n. E igual que antes, el procedimiento de randomizaci√≥n indicado en el c√≥digo debe ser id√©ntico al que se utiliz√≥ para asignar el tratamiento en primer lugar.

Para declarar el **procedimiento de randomizaci√≥n** se usa la funci√≥n `ri2::declare_ra`. En este caso (igual que en el ejemplo de antes) estamos declarando la asignaci√≥n aleatoria de 4 unidades tratadas (`m = 4`) dentro de una muestra de 8 unidades (`N = 8`).

```{r}
declaration <- declare_ra(N = 8, m = 4)
declaration
```

El output de `declare_ra` se guarda en un objeto de R, que luego podremos pasar a la funci√≥n `ri2::conduct_ri`, que es la que realiza el procedimiento de Randomization Inference en s√≠ mismo.

Para declarar el **estad√≠stico del test** tenemos 2 opciones.

Una es hacerlo mediante el argumento `formula` de `conduct_ri`. Si nuestro estad√≠stico es simplemente la diferencia de medias entre tratados y controles, la f√≥rmula ser√° algo como `y ~ d` (con `y` como variable de outcome y `d` como variable de asignaci√≥n de tratamiento).

Alternativamente, podemos especificar el estad√≠stico mediante el argumento `test_function` (tambi√©n de `conduct_ri`), el cual admite cualquier funci√≥n de R capaz de recibir un dataframe y retornar un escalar (i.e., el valor del estad√≠stico de test).

```{r}
# Declarando la `test_function`
sdo <- function(data) {
  data %>% 
  summarise(te1 = mean(y[d == 1], na.rm=TRUE),
            te0 = mean(y[d == 0], na.rm=TRUE),
            sdo = te1 - te0) %>% 
    pull(sdo)
}
```

La **hip√≥tesis nula *sharp*** se especifica mediante el argumento `sharp_hypothesis` de `conduct_ri`. Su valor por defecto es 0, as√≠ que si vamos a usar la *sharp* null de Fischer podemos omitirlo.

Otro detalle importante es que debemos indicar los nombres de las columnas correspondientes al outcome y al tratamiento en el dataset, usando los argumentos `assignment` y `outcome`. Y tambi√©n hay que pasar el dataset en s√≠ mismo con el argumento `data`.

Finalmente, ejecutamos la funci√≥n `conduct_ri` con todos los argumentos necesarios e inspeccionamos los resultados con la funci√≥n `summary`.

```{r}
ri2_out <- conduct_ri(
  test_function = sdo,
  assignment = "d",
  outcome = "y",
  declaration = declaration,
  sharp_hypothesis = 0,
  data = ri
)

summary(ri2_out)
```

Como vemos, el p-value retornado por `conduct_ri` es id√©ntico al que obtuvimos ejecutando el c√≥digo de la secci√≥n anterior, pero con la ventaja de que el c√≥digo de `ri2` es mucho m√°s conciso y legible.

Otra ventaja de `ri2::conduct_ri` es que su output puede ser graficado directamente con la funci√≥n `plot()`.

```{r, message=FALSE}
plot(ri2_out)
```

Este gr√°fico es incluso mejor que el creado a mano en la secci√≥n anterior porque destaca autom√°ticamente los estad√≠sticos que son tan o m√°s extremos que el estad√≠stico simulado, y representa apropiadamente los tests de dos colas sin tener que transformar los estad√≠sticos a valor absoluto ‚ú®.

Adem√°s, dado que el output de `plot(ri2_out)` es un objeto de ggplot2, podemos personalizar el gr√°fico usando las funciones de ggplot2 que ya conocemos. Por ejemplo, podemos aplicarle un tema de `ggthemes`:

```{r, warning=FALSE}
plot(ri2_out) +
  ggthemes::theme_stata()
```

Si queremos inspeccionar manualmente la data de las simulaciones, es posible hacerlo extrayendo el dataframe `sims_df` desde el output de `ri2::conduct_ri`. Este dataframe tendr√° tantas filas como permutaciones de $D$ se hayan efectuado.

```{r}
ri2_out[["sims_df"]] %>% head()
```

Vale la pena mencionar adem√°s que `conduct_ri` tiene un argumento `sims` que indica la **cantidad de permutaciones** a realizar para obtener el p-value. Por defecto su valor es 1000, pero podemos querer incrementarlo en caso de que deseemos obtener una aproximaci√≥n m√°s precisa.

### Bonus: usando el estad√≠stico KS para medir diferencias en distribuciones de outcomes

Una de las ventajas de RI mencionadas al comienzo de este post era la posibilidad de utilizar estad√≠sticos alternativos, siendo el √∫nico requisito sobre estos el que correspondan a valores escalares obtenidos a partir de $D$ e $Y$.

Uno de los estad√≠sticos "alternativos" m√°s interesantes (en mi opini√≥n) es el **estad√≠stico KS** (Kolmogorov-Smirnov) que mide la diferencia m√°xima entre dos funciones de distribuci√≥n acumuladas emp√≠ricas (en este caso, entre la de las unidades tratadas $\hat{F_T}$ y la de las unidades de control $\hat{F_C}$):

$$
T_{KS}=\max|\hat{F_T}(Y_i) - \hat{F_C}(Y_i)|
$$

Lo cual se puede apreciar visualmente en el siguiente gr√°fico de ejemplo, donde el valor del estad√≠stico corresponde al largo de la l√≠nea roja punteada.

![*Ejemplo de estad√≠stico KS con 2 funciones de distribuci√≥n acumuladas. Fuente: [StackExchange](https://stats.stackexchange.com/questions/208517/kolmogorov-smirnov-test-vs-t-test)*](images/ks_example.png)

**¬øPor qu√© querr√≠amos usar el estad√≠stico KS en vez de algo m√°s familiar, como la diferencia de medias?**

Porque puede haber casos en que el tratamiento afecte la distribuci√≥n de los outcomes, pero sin generar cambios importantes en la media, por ejemplo aumentando la dispersi√≥n, o generando una distribuci√≥n bimodal. He aqu√≠ un ejemplo ([tomando del Mixtape](https://github.com/scunning1975/mixtape/blob/master/R/ks.R)) con datos simulados:

```{r, warning=FALSE}
tb <- tibble(
  d = c(rep(0, 20), rep(1, 20)),
  y = c(0.22, -0.87, -2.39, -1.79, 0.37, -1.54, 
        1.28, -0.31, -0.74, 1.72, 
        0.38, -0.17, -0.62, -1.10, 0.30, 
        0.15, 2.30, 0.19, -0.50, -0.9,
        -5.13, -2.19, 2.43, -3.83, 0.5, 
        -3.25, 4.32, 1.63, 5.18, -0.43, 
        7.11, 4.87, -3.10, -5.81, 3.76, 
        6.31, 2.58, 0.07, 5.76, 3.50)
)

kdensity_d1 <- tb %>%
  filter(d == 1) %>% 
  pull(y)
kdensity_d1 <- density(kdensity_d1)

kdensity_d0 <- tb %>%
  filter(d == 0) %>% 
  pull(y)
kdensity_d0 <- density(kdensity_d0)

kdensity_d0 <- tibble(x = kdensity_d0$x, y = kdensity_d0$y, d = 0)
kdensity_d1 <- tibble(x = kdensity_d1$x, y = kdensity_d1$y, d = 1)

kdensity <- full_join(kdensity_d1, kdensity_d0,
                      by = c("x", "y", "d"))
kdensity$d <- as_factor(kdensity$d)

ggplot(kdensity)+
  geom_point(size = 0.3, aes(x,y, color = d))+
  xlim(-7, 8)+
  scale_color_discrete(labels = c("Control", "Treatment"))
```

En el ejemplo de arriba, si hacemos randomization inference usando el SDO como estad√≠stico no podemos descartar la *sharp* null de que el efecto del tratamiento es cero para todas las unidades, a pesar de que claramente las distribuciones son muy distintas.

```{r}
set.seed(1989)

conduct_ri(
  test_function = sdo,
  assignment = "d",
  outcome = "y",
  declaration = declare_ra(N = 40, m = 20),
  sharp_hypothesis = 0,
  data = tb,
  sims = 10000
)
```

Veamos qu√© resultado obtenemos al usar el estad√≠stico KS en vez del SDO:

```{r}
# Declarando la funci√≥n que obtiene el estad√≠stico KS a partir de un dataframe
ks_statistic <- function(data) {
  
  control <- data[data$d == 0, ]$y
  treated <- data[data$d == 1, ]$y
  
  cdf1 <- ecdf(control) 
  cdf2 <- ecdf(treated)
  
  minMax <- seq(min(control, treated),
                max(control, treated),
                length.out=length(c(control, treated))) 
  
  x0 <-
    minMax[which(abs(cdf1(minMax) - cdf2(minMax)) == max(abs(cdf1(minMax) - cdf2(minMax))))]
  
  y0 <- cdf1(x0)
  y1 <- cdf2(x0) 
  
  diff <- unique(abs(y0 - y1))
  
  diff
  
}

# obteniendo el p-value con randomization inference
set.seed(1989)

ri_ks <-
  conduct_ri(
    test_function = ks_statistic,
    assignment = "d",
    outcome = "y",
    declaration = declare_ra(N = 40, m = 20),
    sharp_hypothesis = 0,
    data = tb,
    sims = 10000
  )

ri_ks
```

El estad√≠stico KS s√≠ permite rechazar la nula de efecto 0 a un nivel de significancia del 5%[^5].

[^5]: Por supuesto, es mala idea repetir el test con m√∫ltiples estad√≠sticos hasta encontrar uno que permita rechazar la nula, ya que eso constituir√≠a [p-hacking](https://en.wikipedia.org/wiki/Data_dredging). El estad√≠stico a usar deber√≠a, creo, ser parte del dise√±o del experimento.

En la siguiente figura podemos visualizar el estad√≠stico KS correspondiente al vector $D$ verdadero, junto con las CDF emp√≠ricas de los grupos de tratamiento y control.

```{r}
tb2 <- tb %>%
group_by(d) %>%
arrange(y) %>%
mutate(rn = row_number()) %>%
ungroup()


cdf1 <- ecdf(tb2[tb2$d == 0, ]$y) 
cdf2 <- ecdf(tb2[tb2$d == 1, ]$y) 

minMax <- seq(min(tb2$y),
              max(tb2$y),
              length.out=length(tb2$y)) 

x0 <- 
  minMax[which(abs(cdf1(minMax) - cdf2(minMax)) == max(abs(cdf1(minMax) - cdf2(minMax))) )] 

y0 <- cdf1(x0) 
y1 <- cdf2(x0) 

ggplot(tb2) +
  geom_step(aes(x=y, y=rn, color=factor(d)),
            size = 1.5, stat="ecdf") +
  labs(y = "Densidad") +
  geom_segment(aes(x = x0[1], y = y0[1], xend = x0[1], yend = y1[1]),
               linetype = "dashed", color = "red", size = 1) +
  geom_point(aes(x = x0[1] , y= y0[1]), color="red", size=4) +
  geom_point(aes(x = x0[1] , y= y1[1]), color="red", size=4)
```

## Consideraciones y referencias adicionales üîçüìö

Para concluir, quiero comentar algunos detalles adicionales y consideraciones respecto a esta metodolog√≠a:

-   Para obtener **intervalos de confianza** con RI podemos crear un vector con un rango de valores posibles para el estad√≠stico del test (ej, 0.1, 0.2, 0.3, etc) y usar cada uno de ellos como *sharp* null mediante iteraci√≥n. El intervalo de confianza estar√° compuesto de todos aquellos valores para los cuales la *sharp* null no se rechace.

-   En el *Mixtape* se nos advierte que randomization inference presenta cierto **sesgo en contra de los efectos peque√±os cuando se usa junto a la sharp null de Fischer**. Esto debido a que solo treatment effects relativamente grandes permitir√°n rechazar esta nula a niveles convencionales de significancia. Por ende, si el efecto esperado de nuestro tratamiento es peque√±o, nos conviene agregar *covariates* que expliquen la variable de outcome (esto se puede hacer con el argumento `formula` de `ri2::conduct_ri`), o directamente no usar RI.

-   Otra cr√≠tica que existe hacia esta metodolog√≠a es que, seg√∫n [Andrew Gelman de Columbia University](http://www.stat.columbia.edu/~gelman/), [la *sharp* null representa una hip√≥tesis "poco interesante y acad√©mica"](https://youtu.be/XUh8KsaWfJ4?t=546), ya que la idea de un efecto constante para todas las unidades ser√≠a demasiado restrictiva. En respuesta a eso, [Xinran Li de UIUC plantea](https://youtu.be/XUh8KsaWfJ4?t=807) la idea de generalizar el procedimiento para obtener cuantiles de efectos individuales, y as√≠ llegar a conclusiones tales como "el X% de las unidades experimenta un efecto causal superior a Y".

-   Existe un paquete alternativo a `ri2` para realizar Randomization Inference en R, denominado [`ritest`](http://grantmcdermott.com/ritest/index.html). Es un port de un comando de STATA con el mismo nombre, por lo que puede ser de inter√©s para quienes est√©n m√°s familiarizados con STATA.

Finalmente, compartirles un par de **referencias** en caso de que deseen profundizar m√°s.

-   [La vignette del paquete `ri2`](https://cran.r-project.org/web/packages/ri2/vignettes/ri2_vignette.html) contiene ejemplos de c√≥mo usar sus funciones para dise√±os experimentales m√°s complejos, tales como cuando se prueban varios tratamientos al mismo tiempo (multi-arm design), se desea evaluar interacciones con covariables, o se hace randomizaci√≥n con clustering o blocking por grupos.

-   [Este apunte de Matthew Blackwell](https://www.mattblackwell.org/files/teaching/s05-fisher.pdf) resume muy bien los conceptos de randomization inference, por lo que resulta buen material de consulta o repaso (de hecho me apoye en √©l para escribir este art√≠culo, junto con [el cap√≠tulo 4 de Causal Inference: The Mixtape](https://mixtape.scunning.com/potential-outcomes.html#randomization-inference)).

-   El [cap√≠tulo 5 de *Causal Inference for Statistics, Social, and Biomedical Sciences*](https://www.cambridge.org/core/books/abs/causal-inference-for-statistics-social-and-biomedical-sciences/fishers-exact-pvalues-for-completely-randomized-experiments/23AF990D2EF9C90D0A424D555FACE578) de Imbens y Rubin (2015) entra en m√°s detalle en varios t√≥picos de Randomization Inference, en especial en lo que respecta a los estad√≠sticos de test. 

*Tu feedback es bienvenido. Si tienes comentarios sobre este art√≠culo puedes envi√°rmelos [por correo](mailto:francisco.yira@outlook.com).*
