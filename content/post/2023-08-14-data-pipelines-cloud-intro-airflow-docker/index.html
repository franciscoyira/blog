---
title: "A Beginner's Introduction to Airflow with Docker ‚Äî Data Pipelines in the Cloud (I)"
author: Francisco Yir√°
date: '2023-08-13'
slug: data-pipelines-cloud-intro-airflow-docker
useRelativeCover: true
cover: "images/cover.jpeg"
coverCaption: "My attempt of using Stable Diffussion to depict something cloud-computery."
description: Learn the essentials of Apache Airflow for creating scalable and automated data pipelines in the cloud with this comprehensive, step-by-step beginner‚Äôs guide. Discover what problem Airflow solves and under what circumstances is better to use it and run your first Airflow DAG on Docker with the Linux subsystem for Windows.
categories:
  - data-science
  - tutorial
  - cloud
  - data-engineering
tags:
  - airflow
  - docker
  - data-pipelines
---



<p>After a long hiatus (sorry about that üò≥), I‚Äôm excited to post again on this blog and share with you what I‚Äôve been learning recently: how to use Apache Airflow and Docker to build scalable data pipelines that interact with AWS cloud services, such as S3 (object storage) and RDS (relational databases).</p>
<p>If you‚Äôre new to Airflow and curious about what it‚Äôs good for (maybe you‚Äôve heard some data engineers talking about it or have seen it on data science job postings?), then this will be a great place to start, because the post won‚Äôt assume any previous knowledge about it.</p>
<p>By the end of this post, you will:</p>
<ul>
<li>Understand what Airflow is used for, what its main capabilities are, and how to tell if it‚Äôs a suitable solution for your data pipelines.</li>
<li>Know how to install and run Airflow on a local environment through Docker.</li>
<li>Know how to create and schedule simple DAGs and tasks.</li>
<li>Have some familiarity with the core components of the Airflow UI, including how to monitor the status of your DAGs.</li>
</ul>
<p>One thing that I‚Äôll have to assume, though, is that you have some basic familiarity with the Python and Docker, but I‚Äôll also provide links to introductory resources on them that you can refer to if you feel lost at some point üôÇ.</p>
<blockquote>
<p><strong><em>Data Pipelines in the Cloud Series</em></strong></p>
<ul>
<li><p><strong>Part I: You‚Äôre here.</strong></p></li>
<li><p><a href="/post/aws-command-line-data-pipelines-cloud-part-2/">Part II: Using Amazon Web Services with the Command Line</a></p></li>
<li><p><em>Part III: Building an Airflow Pipeline That Talks to AWS. (Coming soon! <a href="/subscribe">Subscribe to get notified</a>)</em></p></li>
</ul>
</blockquote>
<div id="what-problem-does-airflow-solve" class="section level2">
<h2>What problem does Airflow solve?</h2>
<p><em>You can skip this part if you already know what Airflow is and just want to learn to use it.</em></p>
<p>If you‚Äôve been working on data science for some time, you‚Äôve probably encountered the need to set up automatic, scheduled pipelines. Typical examples of this are retrieving data from an API on daily basis or loading scores from a predictive model to your company‚Äôs database.</p>
<p>For the simplest pipelines, this can be achieved through a Cron Job<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> or even using the Windows‚Äô Task Scheduler. If your pipeline consists only of SQL queries, you can also leverage the <a href="https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-schedule-query.html">scheduling features available on some databases</a>.</p>
<p>However, as your pipelines grow in number and complexity, you may reach a point where these tools are no longer enough. Some signals that you‚Äôve reached this threshold are:</p>
<ul>
<li><p>You need detailed logs for most of your pipelines but you‚Äôre setting up the logging system ‚Äúmanually‚Äù for each one of them. Even worse, each of the data scientists on your team has implemented their own logging mechanism.</p></li>
<li><p>Some of your pipelines are very intensive on compute and would benefit from being distributed on a scalable infrastructure (i.e.¬†run on more computers).</p></li>
<li><p>Other processes must run before your pipeline, but their end time is variable or unknown (you need to ‚Äúdetect‚Äù or ‚Äúlisten to‚Äù when they‚Äôre done).</p></li>
<li><p>Your pipelines are becoming more critical in your organisation so you‚Äôre starting to develop a dashboard to check their status and receive alerts when something goes wrong.</p></li>
</ul>
<p>All of these involve using some features (logging, scalability, dependencies, monitoring, etc) that you <em>technically</em> can code on your own, but why waste on that time if someone else already has done this for you? <strong>This is where Airflow comes in handy</strong>.</p>
<div id="airflow-the-orchestrator-of-data-pipelines" class="section level3">
<h3>Airflow, the orchestrator of data pipelines</h3>
<p><img src="images/airflow_orchestra.jpg" /></p>
<p>Apache Airflow can be defined as an <strong>orchestrator for complex data flows</strong>. Just like a music conductor coordinates the different instruments and sections of an orchestra to produce harmonious sound, Airflow coordinates your pipelines to make sure they complete the tasks you want them to do, even when they depend on each other in very complex ways.</p>
<p>The analogy falls short, however, in depicting the full range of Airflow benefits: it‚Äôs not only the coordination but also the <strong>monitoring</strong>, <strong>logging</strong> and <strong>scaling</strong>.</p>
<p>For example, Airflow provides an extremely thorough web UI that allows you to <strong>monitor</strong> the status and history of your pipelines, sparing you the need to develop your own dashboard.</p>
<div class="figure">
<img src="images/airflow_ui.png" alt="Airflow UI: Tree View of a single pipeline that allows you to see the state and history of each of the tasks in a pipeline and even how they relate to each other!" />
<p class="caption"><em>Airflow UI: Tree View of a single pipeline that allows you to see the state and history of each of the tasks in a pipeline and even how they relate to each other!</em></p>
</div>
<p>The same UI gives you access to <strong>detailed logs</strong>, which can be used to to debug your pipeline‚Äôs code when a task is failing. You can also receive <strong>mail</strong> <strong>alerts</strong> when a task fails or exceeds a certain duration, and configure Airflow to automatically <strong>re-try tasks</strong> that fail under certain conditions. And when fixing the error requires manually modifying your code, you can <strong>re-run only the failing tasks</strong> and their downstream dependencies, while keeping the upstream tasks untouched, thus saving time and processing power.</p>
<div class="figure">
<img src="images/logs_airflow.png" alt="Accessing tasks‚Äô logs on Airflow‚Äôs UI. Again, this is something you would have to implement by yourself if you used Cron Jobs, but which you get for free with Airflow." width="850" />
<p class="caption"><em>Accessing tasks‚Äô logs on Airflow‚Äôs UI. Again, this is something you would have to implement by yourself if you used Cron Jobs, but which you get for free with Airflow.</em></p>
</div>
<p>Airflow also helps you <strong>better manage your computing resources</strong> by setting up a queue of tasks that get assigned to the available computers (called nodes or workers), and by letting you <strong>scale up or down</strong> the pool of nodes dedicated to your tasks. This allows you to run your pipelines faster or to lower your cloud computing bill, depending on your needs</p>
{{% subscribe %}}
<p>Finally, with Airflow you also get <strong>extensibility</strong>, which makes it easy to re-use code across pipelines and teams. For example, a data engineer on your team can create a parametrised <em>Operator</em> (more on them later!) that performs a frequent task on your company‚Äôs data warehouse. Then you can just reuse that operator instead of having to code the task on your own. Even better, you can find ready-to-use extensions on the <a href="https://airflow.apache.org/plugins.html">Airflow plugin repository</a> so most of the time nobody in your team will have to write anything from scratch.</p>
</div>
<div id="do-you-really-need-airflow" class="section level3">
<h3>Do you <em>really</em> need Airflow?</h3>
<p>All of this sounds pretty neat, right? You may even be wondering why anyone would not want to use Airflow for their data pipelines. Here the music director analogy becomes appropriate again: <strong>you wouldn‚Äôt hire a music conductor to direct a solo performance or a small band</strong>. If your pipeline involves few steps and has no complex dependency logic, then you‚Äôll spend more time and resources setting up Airflow than you‚Äôll gain from using it, and you‚Äôll be better off with a simpler tool, like a Cron Job.</p>
<p>One way to think about Airflow is as a <strong>lesser evil</strong>: you pay up-front for a lot of complexity <del>and annoyance</del> in order to get a more scalable and robust system on which each additional pipeline or component adds less marginal complexity.</p>
<div class="figure">
<img src="images/airflow_complexity.png" alt="This graph is intentionally drawn by hand so you don‚Äôt take it too seriously." />
<p class="caption"><em>This graph is intentionally drawn by hand so you don‚Äôt take it too seriously.</em></p>
</div>
</div>
</div>
<div id="how-to-declare-our-pipelines-in-airflow-introducing-dag.py" class="section level2">
<h2>How to declare our pipelines in Airflow: introducing <em>dag.py</em></h2>
<p>Okay, now we know what Airflow is and what it‚Äôs good for. Time to learn how to use it!</p>
<p>First, we need to learn how to declare our pipelines in Airflow. For this we use a concept named <strong>DAG</strong> or <strong>Directed Acyclic Graph</strong><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>. A DAG is a <em>graph</em> with <strong>nodes</strong> and <strong><em>directed</em></strong> <strong>edges</strong> (arrows) that represent the tasks and dependencies in our pipelines. It is ‚Äúacyclic‚Äù because it doesn‚Äôt allow loops or cycles: <strong>you can‚Äôt plug the output of a task back into an upstream dependency</strong> (it would be a paradox as if Marty McFly from Back to Future met his younger self in 1955).</p>
<p><img src="images/dags_vs_nodags.png" /></p>
<p>In the context of Airflow, the nodes of the DAG represent tasks to performed, and the edges or arrows represent the relationships of dependency between those tasks.</p>
<p><img src="images/airflow_dag.jpg" /></p>
<p>How do we declare these DAGs in our code? Here is a basic example:</p>
<pre class="python language-python"><code>from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta

with DAG(
    &quot;tutorial2&quot;,
    description=&quot;A simple tutorial DAG&quot;,
    schedule_interval=timedelta(days=1),
    start_date=datetime(2021, 1, 1),
    catchup=False,
    tags=[&quot;example_tag&quot;],
) as dag:
    
    # Task 1
    t1 = BashOperator(
        task_id=&quot;print_date&quot;,
        bash_command=&quot;date&quot;,
    )

    # Task 2
    t2 = BashOperator(
        task_id=&quot;sleep&quot;,
        bash_command=&quot;sleep 5&quot;,
        retries=3,
    )

    # Task 3
    t3 = BashOperator(
        task_id=&quot;echo_execution_date&quot;,
        bash_command=&quot;echo {{ ds }}&quot;,
    )

    t1 &gt;&gt; [t2, t3]</code></pre>
<p>Let‚Äôs explain it line by line. First, we import the necessary libraries:</p>
<pre class="python language-python"><code>from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta</code></pre>
<p>We always need to import the <code>DAG</code> class when using Airflow, but what comes after that depends on what <em>our</em> DAG is going to do.</p>
<p>In this case, we import <strong><code>BashOperator</code></strong>. You can think of <em>Operators</em> as blue prints or building blocks for tasks. By importing this <code>BashOperator</code>, we can create tasks that perform actions on the Bash shell (a Unix command line tool), but there are many more operators that allow to perform other kinds of actions.</p>
<p>Finally, we import a couple of <code>datetime</code> functions. These are common imports on Airflow code since they help us to declare the schedule on which the DAG will run.</p>
<p>Then, we declare the DAG itself:</p>
<pre class="python language-python"><code>with DAG(
    &quot;tutorial&quot;,
    description=&quot;A simple tutorial DAG&quot;,
    schedule_interval=timedelta(days=1),
    start_date=datetime(2021, 1, 1),
    catchup=False,
    tags=[&quot;example_tag&quot;],
) as dag:</code></pre>
<p>For this we use the constructor of the <code>DAG</code> class to create a <code>dag</code> object<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. Here we can specify some parameters or options of the DAG we‚Äôre creating.</p>
<ul>
<li><p>The first line indicates the <em>name</em> of the DAG (<em>‚Äútutorial‚Äù</em>, in this case). This name must be a string, unique across your Airflow instance and can‚Äôt contain spaces or special characters (but it does allow underscores, dashes, dots and numbers). This name will appear next to your DAG in the Airflow UI and is also used for searching, so it‚Äôs a good idea to make it descriptive and memorable (but not too long).</p></li>
<li><p>The second line is the <em>description</em>. Here you can go into more detail about what your DAG does.</p></li>
<li><p><code>schedule_interval</code> specifies the schedule on which your DAG runs. In this case, <code>timedelta(days=1)</code> defines a daily schedule: the DAG will run every 24 hours after the <code>start_date</code>. Some other common schedule intervals are <code>timedelta(hours=12)</code> for twice daily, <code>timedelta(weeks=1)</code> for once a week and <code>timedelta(minutes=30)</code> for every 30 minutes. You can also use <a href="https://crontab.guru/">Cron expressions</a> for this argument.</p></li>
<li><p><code>start_date</code> indicates the moment when the DAG becomes active (i.e.¬†when the execution schedule begins). If <code>start_date</code> is in the future, the DAG will wait until that moment to run for the first time. If it‚Äôs in the past, then what happens depends on the <code>catchup</code> parameter, since you may have some <em>missed executions.</em> For example, with a <em>daily</em> execution schedule and a <code>start_date</code> set to 7 days ago, you would have <em>7 missed executions</em>. If <code>catchup=True</code>, Airflow will try to run those missing executions (a process known as <em>backfilling)</em>. If it‚Äôs <code>catchup=False</code>, Airflow will skip those past executions and only try to run the next scheduled execution.</p></li>
</ul>
<div class="figure">
<img src="images/airflow_catchup.png" alt="Diagram illustrating how catchup works. In this example, start_date is set to the beginning of 2019 but the current date is in the middle of 2022. Since the execution schedule is yearly, we have 3 missed executions, which Airflow will try to run if catchup=True." />
<p class="caption"><em>Diagram illustrating how catchup works. In this example, start_date is set to the beginning of 2019 but the current date is in the middle of 2022. Since the execution schedule is yearly, we have 3 missed executions, which Airflow will try to run if catchup=True.</em></p>
</div>
<ul>
<li><code>tags</code>: this parameter allows you to assign descriptive labels to your DAG, making it easier to filter and search for specific DAGs in the Airflow UI. This can be particularly useful in large environments where many DAGs are in operation, as it enables quick identification of DAGs that relate to specific systems, departments, or functionality (e.g.¬†Marketing, Operations, Compliance, etc).</li>
</ul>
<p>Finally, we declare the tasks that make up the DAG, along with their execution order:</p>
<pre class="python language-python"><code>    # Task 1
    t1 = BashOperator(
        task_id=&quot;print_date&quot;,
        bash_command=&quot;date&quot;,
    )

    # Task 2
    t2 = BashOperator(
        task_id=&quot;sleep&quot;,
        bash_command=&quot;sleep 5&quot;,
        retries=3,
    )

    # Task 3
    t3 = BashOperator(
        task_id=&quot;echo_execution_date&quot;,
        bash_command=&quot;echo {{ ds }}&quot;,
    )

    t1 &gt;&gt; [t2, t3]</code></pre>
<p>In this section, we are declaring three individual tasks, each of them using <code>BashOperator</code> as <em>blue print</em><a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>.</p>
<p>All tasks utilise the <code>task_id</code> argument to define a name. This name should be descriptive, as it will show up next to the task in the Airflow UI. It also must be unique <em>within the context of a particular DAG</em>, but you can have two tasks with the same name in different DAGs. Note that each task also has a variable name (t1, t2, etc). This name is used only within the context of the DAG script and don‚Äôt need to match the <code>task_id.</code> However, it‚Äôs a good idea to make them the same can for the sake of clarity and readability.</p>
<p>Another argument that is used by all these tasks is <code>bash_command</code>. This argument is specific to tasks that use the <code>BashOperator</code>. It literally receives the commands that will be executed on the Bash terminal during the task‚Äôs run.</p>
<p>In this example, every task passes different commands through this argument, thus defining the specific actions performed by each one:</p>
<ul>
<li><code>t1</code> (<code>'print_date'</code>): Prints the current date using the UNIX <code>date</code> command.</li>
<li><code>t2</code> (<code>sleep</code>): Executes the <code>sleep 5</code> command, which pauses the process for 5 seconds.</li>
<li><code>t3</code> (<code>echo_execution_date</code>): Prints the execution date of the DAG by using <code>{{ ds }}</code>, a special template field provided by Airflow that represents the execution date in the format <code>YYYY-MM-DD</code><a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>.</li>
</ul>
<p>Something cool about the <code>BashOperator</code> is that it can be used for executing almost any command that runs on a Bash terminal: moving, renaming or deleting files, installing or updating packages, running Python or R scripts, generating Quarto reports, etc. While some of these tasks may benefit from custom Operators specifically designed for them (e.g.¬†<code>PythonOperator</code>), it‚Äôs nice to know that <strong>if you can run something on Bash, then you can automate it with Airflow</strong>.</p>
<p>Finally, the line <code>t1 &gt;&gt; [t2, t3]</code> sets up the <strong>execution order</strong>, indicating that Task 1 (t1) must be followed by both Task 2 (t2) and Task 3 (t3). Tasks t2 and t3 being contained in a list <code>[...]</code> means that they both run in parallel after t1 completes.</p>
</div>
<div id="how-to-install-airflow-and-run-the-dag" class="section level2">
<h2>How to install Airflow and run the DAG</h2>
<p>Okay, so now we have a minimal <em>dag.py</em> script that declares a DAG with several tasks, specifying dependencies between them. But how can we make this pipeline run?</p>
<p>To make our script work, we need to install and set up an Airflow environment. There are many ways to do this, but I‚Äôll go ahead and explain <strong>how to do it on the Windows Subsystem for Linux (WSL) with Docker</strong>. Why? Because this way the instructions work for most people.</p>
<ul>
<li>If you use Windows, the instructions are literally meant for you.</li>
<li>If you use Linux, you just run the Bash commands and skip the parts about setting up WSL, since you‚Äôre running Linux already.</li>
<li>If you use Mac, 95% of the Ubuntu Bash instructions should work for you, since both operating systems are Unix-like.</li>
</ul>
<p>Also, proficiency with <strong>Docker</strong> is another valuable skill if we‚Äôre planning to immerse ourselves in the realms of data engineering and cloud computing.</p>
<div class="figure">
<img src="images/realms_data_emgineering.jpeg" alt="The realms of data engineering and cloud computing." width="650" />
<p class="caption"><em>The realms of data engineering and cloud computing.</em></p>
</div>
<p>While we could technically run Airflow directly on WSL/Linux without Docker, by leveraging Docker features, such as Docker Compose files (which are like cooking recipes for software environments), we can ensure the reproducibility of our environment and avoid the hassle of setting up each Airflow component manually. <strong>By defining the entire process using these ‚Äòrecipes,‚Äô we get a consistent set of instructions that reduces the chance of errors and ensures we‚Äôll end up with the same Airflow environment wherever we execute those instructions</strong><a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>.</p>
<p>So, off we go!</p>
<div id="installing-airflow-on-the-windows-subsystem-for-linux" class="section level3">
<h3>Installing Airflow on the Windows Subsystem for Linux</h3>
<p><img src="images/penguin_pinwheel_window.jpeg" width="650" /></p>
<p>Before we install Airflow this way, we need to make sure that we have the following prerequisites:</p>
<ul>
<li><p><strong>Windows 10 version 2004 or higher</strong> (to run WSL 2.0).</p></li>
<li><p><strong>WSL 2.0 enabled and configured</strong>. If you don‚Äôt have this already, you can just open PowerShell or Windows Command Prompt in administrator mode (by right-clicking and selecting ‚ÄúRun as administrator‚Äù), then enter the <code>wsl --install</code> and then restart your machine.</p></li>
<li><p><strong>Ubuntu 20.04 LTS installed on WSL 2.0.</strong> After enabling WSL you can install it <a href="https://apps.microsoft.com/store/detail/ubuntu-20046-lts/9MTTCL66CPXJ?hl=en-us&amp;gl=us">directly from the Microsoft Store</a>. Then it will appear as an app on your Start Menu. You just click on it and follow the instructions to configure your UNIX user account<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>.</p></li>
</ul>
<div class="figure">
<img src="images/launching_ubuntu_windows.png" alt="Launching Ubuntu on Windows 11" />
<p class="caption"><em>Launching Ubuntu on Windows 11</em></p>
</div>
<p>Next, we launch the Ubuntu Bash and run the following commands (for updating the system and <code>python3</code> and installing <code>pip</code>).</p>
<pre class="bash"><code>sudo apt update &amp;&amp; sudo apt upgrade
sudo apt upgrade python3
sudo apt install python3-pip</code></pre>
<p>Next, we‚Äôll need to install <a href="https://www.docker.com/products/docker-desktop/"><strong>Docker for Windows</strong></a> (or the appropriate version for macOS or Linux if that‚Äôs what we‚Äôre running). If we‚Äôre using Windows, Docker should automatically recognise and integrate with the Ubuntu distro that we‚Äôve installed through WSL.</p>
<p>After installing Docker, it is recommended to restart your Windows system. Once restarted, just launch Docker and then open the Ubuntu Bash again. There you can run the following command to check that the integration between WSL and Docker is working properly:</p>
<pre class="bash"><code>sudo service docker start</code></pre>
<p><em>(This command should also run on Linux and macOS if you installed Docker successfully on your system).</em></p>
<p>If you get an error, it may be helpful to check the following Settings within the Docker UI on Windows:</p>
<p><img src="images/docker_wsl_integration.png" /></p>
<p>Then, we proceed to create a specific directory structure on Ubuntu for our Airflow setup:</p>
<pre class="bash"><code>mkdir Airflow &amp;&amp; cd Airflow</code></pre>
<p>This line combines two commands:</p>
<ul>
<li><p><code>mkdir Airflow</code>: This creates a new directory named ‚Äú<code>Airflow</code>.‚Äù</p></li>
<li><p><code>cd Airflow</code>: This changes the current working directory to the newly created ‚Äú<code>Airflow</code>‚Äù directory.</p></li>
</ul>
<p>By combining these with <code>&amp;&amp;</code>, we ensure that the second command only runs if the first is successful.</p>
<p>Then, we creates three directories: ‚Äú<code>dags</code>‚Äù, ‚Äú<code>logs</code>,‚Äù and ‚Äú<code>plugins</code>‚Äù, inside the current working directory (here the <code>-p</code> option ensures that the command will not produce an error if the directories already exist). These directories correspond to the typical folder structure of an Airflow environment.</p>
<pre class="bash"><code>mkdir -p ./dags ./logs ./plugins</code></pre>
<p>We then execute the following command:</p>
<pre class="bash"><code>echo -e &quot;AIRFLOW_UID=$(id -u)&quot; &gt; .env</code></pre>
<p>This line is a bit more complex, but suffices to say that it creates a file named <code>.env</code> and writes into it a user ID value related to Airflow‚Äôs setup. This ensures the system can access this value later.</p>
<p>Next, we move on to <strong>downloading the necessary Docker Compose file (</strong>using the <code>curl</code> command) <strong>and initialising the Airflow service</strong>. As I said before, you can think of a Docker Compose file as some sort of ‚Äúrecipe‚Äù to create and configure all the necessary components of a software application, ensuring that they work together in harmony. This specific <code>docker-compose.yaml</code> file outlines the instructions for setting up Airflow version 2.4.0.</p>
<pre class="bash"><code>curl -LfO &#39;https://airflow.apache.org/docs/apache-airflow/2.4.0/docker-compose.yaml&#39;</code></pre>
<p>Then, the following command starts the <code>airflow-init</code> service defined in the Docker Compose file:</p>
<pre class="bash"><code>docker compose up airflow-init</code></pre>
<ul>
<li><p><code>docker compose up</code> starts the services defined in the <code>docker-compose.yaml</code> file.</p></li>
<li><p><code>airflow-init</code> is the specific service we want to start, responsible for initialising Airflow.</p></li>
</ul>
<p>Once the initialisation is complete, we can finally start the Airflow services and verify them with the following commands:</p>
<pre class="bash"><code>docker compose up -d
docker ps</code></pre>
<p>If everything went well, your Ubuntu bash should spit out something like this after running <strong><code>docker ps</code></strong>:</p>
<p><img src="images/airflow_docker_ps.png" />Looks like quite a bit to take in, doesn‚Äôt it? <strong>That‚Äôs because an Airflow environment consists of several components, like the database, scheduler, web UI, and more.</strong> Each one runs in its own container (which you can think of as a lightweight virtual machine), all working together. But all you really need to focus on for now is the <code>STATUS</code> column. If the containers are marked as ‚Äúhealthy,‚Äù everything‚Äôs set up right, and you‚Äôre good to go!<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>.</p>
<p>Now, just navigate to <a href="http://localhost:8080/" class="uri">http://localhost:8080/</a> in your browser, and you‚Äôll be able to access the Airflow UI using <code>airflow</code> as both username and password.</p>
<p>If you see something like the screenshot below, congratulations! You‚Äôre looking at the UI of your very own self-hosted Airflow instance üôåüèΩüëèüèΩü•≥üéâü™Ö</p>
<p><img src="images/airflow_ui_hello.png" /></p>
</div>
<div id="deploying-our-dag" class="section level3">
<h3>Deploying our DAG</h3>
<p>Let‚Äôs wrap things up by deploying our DAG to Airflow. To do this, we simply need to <strong>copy the code of our DAG as a .py file into the ‚Äò<code>dags</code>‚Äô folder</strong> of our Airflow environment.</p>
<p>Since there are various ways to accomplish this, I‚Äôll offer two methods:</p>
<ul>
<li><p><strong>Saving <code>dag.py</code> using the Ubuntu Bash</strong>: This method is quick to set up, although it might be less convenient if you plan on regularly modifying the DAG.</p></li>
<li><p><strong>Saving <code>dag.py</code> using Visual Studio Code:</strong> This may take a bit more effort initially but will make future modifications and ongoing development of the DAG much simpler.</p></li>
</ul>
<div id="saving-dag.py-using-bash" class="section level4">
<h4>Saving <code>dag.py</code> using Bash</h4>
<ol style="list-style-type: decimal">
<li>Copy <a href="assets/dag.py">the DAG‚Äôs code</a>.</li>
<li><strong>Navigate to the <code>dags</code> folder inside the <code>Airflow</code> directory we created earlier.</strong> If you see <code>~/Airflow$</code> on your Bash, you just need to execute this command: <code>cd dags</code>.</li>
<li><strong>Create a subfolder named <code>tutorial_dag</code></strong> with the following command <code>mkdir -p tutorial_dag</code> (it‚Äôs good practice to keep the <code>dags</code> folder tidy by having each DAG in a corresponding subfolder).</li>
<li>Navigate to the <code>tutorial_dags</code> subfolder by executing this command: <code>cd tutorial_dags</code>.</li>
<li><strong>Use the <code>cat</code> command to create the <code>dag.py</code> file.</strong> Execute <code>cat &gt; dag.py</code>.</li>
<li>Paste the entire DAG code you copied in step 1 and press Enter.</li>
<li>Press <code>Ctrl+D</code> to save the file and exit.</li>
</ol>
<p>Done.</p>
<p>After this, we should be able to see our DAG (named <code>tutorial</code>) on the Airflow UI. We can use the Airflow search box for that, it should be the first result so we could just write ‚Äútutorial‚Äù and press Enter.</p>
<p><img src="images/search_airflow.png" /></p>
<p>Then we can see different views of the DAG in the Airflow UI, one of them is the <strong>Graph View</strong>. Here you can see a visual representation of the DAG, what Operator is used by each task, and the current status of each one.</p>
<p><img src="images/graph_view.png" /></p>
<p>Another relevant view is the Code View, where you can see the source code of the DAG (the same code that lives in the <code>dag.py</code> file):</p>
<p><img src="images/code_view.png" /></p>
<p>Here, if you pay close attention, you can notice that we‚Äôve made an error: the DAG we‚Äôre seeing is not our DAG; it‚Äôs an example DAG that came with the Docker image we deployed üòØüòÖ. Since DAG names have to be unique and this DAG was first on the Airflow instance, we can‚Äôt use the name ‚Äò<code>tutorial</code>‚Äô for a new DAG like ours.</p>
<p>This seems like a good opportunity to learn how to edit the DAG files inside WSL, so we can amend mistakes like this one or make further modifications.</p>
</div>
<div id="editing-dag.py-using-visual-studio-code" class="section level4">
<h4>Editing <code>dag.py</code> using Visual Studio Code</h4>
<p>For this to work, you need to first download and install <a href="https://code.visualstudio.com/download">Visual Studio Code</a>, and then install the <a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-wsl">WSL extension</a> (this last step is only necessary if you‚Äôre using Windows; what comes next should work fine on Linux and macOS with VS Code installed).</p>
<p>Now, we go back to Bash and, while still in the <code>Airflow</code> directory, we write <code>code .</code> and press Enter.</p>
<p><img src="images/opening_vs_code_bash.png" width="450" /></p>
<p>VS Code will automatically open the <code>Airflow</code> directory, providing you with a full IDE to edit, create and delete DAG files (and if you‚Äôre using WSL, it will integrate seamlessly with Bash too).</p>
<p>Once here, just navigate to our <code>dag.py</code> file, open it, change the name from ‚Äútutorial‚Äù to ‚Äútutorial2‚Äù (or whatever name you want; it doesn‚Äôt matter) and save it.</p>
<p><img src="images/vs_code_dag.png" /></p>
<p>Now you can you back to the Airflow UI, where, after a couple seconds, a new DAG with the unique name we just defined should appear. The names of the tasks should match the <code>task_id</code>s we defined previously (print_date, sleep and echo_execution_date), and the code you see on the ‚ÄúCode‚Äù tab should also match the code of the dag.py file we just edited.</p>
<p>If everything checks out, then we can initiate the DAG run by pressing the ‚ÄúPlay‚Äù button in the top-right corner and then pressing ‚ÄúTrigger DAG‚Äù.</p>
<p><img src="images/dag_start.png" /></p>
<p>If you‚Äôre in the Graph view, you should then see how each task in the DAG changes colour to light green (indicating it‚Äôs running) and then to dark green (indicating it ran successfully).</p>
<p><img src="images/dag_success.png" /></p>
</div>
</div>
</div>
<div id="we-did-it" class="section level2">
<h2>We did it!</h2>
<p><img src="images/dora.png" title="&#39;Navigated to the dags, folder with such ease, Created tutorial_dags, it was such a breeze, Opened in VS Code, edited the file, Hit the play button, now we&#39;re running in style!&#39; - ChatGPT when I asked for a Dora The Explorer song about Airflow" /></p>
<p>Congratulations! You successfully created and ran your first Airflow DAG using Docker, and you are starting to unlock the power of Airflow to automate your data workflows! üéâ</p>
<p>In this post, you learnt:</p>
<ul>
<li>The basics of why we use Airflow and how it works</li>
<li>How to install and run Airflow using Docker on Ubuntu</li>
<li>How to create and schedule simple DAGs and tasks using Python</li>
<li>How to monitor the status and logs of your DAGs using the Airflow UI</li>
</ul>
<p>Note that if we keep Docker and the Airflow instance running, the <code>tutorial2</code> DAG will run again every day, and we‚Äôll have access to the history, status, and logs of those executions. But in this case, it doesn‚Äôt make much sense because it‚Äôs just a toy DAG that doesn‚Äôt do anything really useful.</p>
<p>In my next post, I will show you <strong>how to build a more functional DAG that performs useful data transformations</strong>, interacting with data sources and services on AWS such as RDS (relational databases) and S3 (object storage). You will learn how to use operators, hooks, sensors, variables, and more. Don‚Äôt miss it!<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a></p>
{{% subscribe %}}
<p><em>If you enjoyed this post, please <a href="https://www.linkedin.com/feed/?shareActive&amp;text=Just%20finished%20reading%20this%20insightful%20guide%20on%20creating%20and%20running%20a%20first%20Airflow%20DAG%20using%20Docker.%20It&#39;s%20packed%20with%20practical%20steps%20and%20tips.%20If%20you&#39;re%20interested%20in%20data%20automation%20and%20Airflow,%20this%20is%20a%20must-read!%20%23ApacheAirflow%20%23DataAutomation%20%0Afranciscoyira.com/post/data-pipelines-cloud-intro-airflow-docker/">share it with your friends and colleagues</a> who might be interested in learning about Airflow. And if you have any questions or feedback, feel free to reach me through <a href="https://www.linkedin.com/in/franciscoyira/">LinkedIn</a> or <a href="https://techhub.social/@franciscoyira">Mastodon</a>. I would love to hear from you. Thanks for reading!</em></p>
<div id="references" class="section level3">
<h3>References üìö</h3>
<ul>
<li><p><a href="https://www.udemy.com/course/the-complete-hands-on-course-to-master-apache-airflow/">The Complete Hands-On Course to Master Apache Airflow</a> by Lamberti, Marc.</p></li>
<li><p><a href="https://medium.com/@ericfflynn/my-journey-with-apache-airflow-d7d364fc84ba">‚ÄúMy Journey with Apache Airflow.‚Äù Medium, 28 Feb.¬†2020</a> by Flynn, Eric.</p></li>
</ul>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>A task scheduler for Unix-based operating systems.<a href="#fnref1" class="footnote-back">‚Ü©</a></p></li>
<li id="fn2"><p>Yes, we‚Äôre talking about DAGs <a href="/post/2021-07-11-diagramas-causalidad-cap-3-causal-inference-mixtape/">again</a>, but not the same kind. Here, DAGs are for managing tasks and workflows, not for modeling causality. Still, Airflow DAGs will be more intuitive to you if you know causal DAGs because both share the same logic.<a href="#fnref2" class="footnote-back">‚Ü©</a></p></li>
<li id="fn3"><p>Fun fact: You‚Äôre not obliged to name this object <code>dag</code>. It can be assigned to any valid variable name. However, it‚Äôs a convention and best practice to name it <code>dag</code> for readability and standardisation reasons, much like predictors are named <code>X</code> and the response variable is named <code>y</code> in <code>sklearn</code>.<a href="#fnref3" class="footnote-back">‚Ü©</a></p></li>
<li id="fn4"><p>Keep in mind that indentation is necessary to tell Python that these tasks are part of the previously declared DAG.<a href="#fnref4" class="footnote-back">‚Ü©</a></p></li>
<li id="fn5"><p>These templates are known as <em>Jinja templates</em> and are especially useful when you want your task to do something using the execution date as a parameter (for example, creating a temporary SQL table that includes the execution date in the name). It‚Äôs important to note that this execution date corresponds to the scheduled execution date, not the actual execution date. How could these two be different? Well, remember the case when you have missing executions, and Airflow tries to catch up on them. For these, the execution date will be the original date on which they were supposed to run, according to the DAG‚Äôs schedule. Another possible scenario is that a DAG fails to run on the scheduled date (e.g., 1st January), and then you attempt to run it again on a later date (5th January). In this case, the execution date will still be 1st January when you try to re-run the DAG on 5th January.<a href="#fnref5" class="footnote-back">‚Ü©</a></p></li>
<li id="fn6"><p>The alternative to Docker Compose would be to execute A LOT of instructions on Bash, line by line. This is a manual and error-prone process that can lead to misconfigurations, version mismatches, and a bunch of other issues that we don‚Äôt want to deal with.<a href="#fnref6" class="footnote-back">‚Ü©</a></p></li>
<li id="fn7"><p>I would also recommend installing the <a href="https://apps.microsoft.com/store/detail/windows-terminal/9N0DX20HK701?hl=es-cl&amp;gl=cl&amp;rtc=1">Windows Terminal</a>, but this is very much optional.<a href="#fnref7" class="footnote-back">‚Ü©</a></p></li>
<li id="fn8"><p>If things didn‚Äôt go well, you can restart the process by running <code>docker compose down</code> to shut down the containers, followed by <code>docker compose up -d</code> to start them again. Finally, check the status once more with <code>docker ps</code>.<a href="#fnref8" class="footnote-back">‚Ü©</a></p></li>
<li id="fn9"><p>In the meanwhile, I recommend you to continue exploring the different ways the Airflow UI lets you monitor your DAGs‚Äô status. Some are more helpful than others, but all are worth a peek.<a href="#fnref9" class="footnote-back">‚Ü©</a></p></li>
</ol>
</div>
