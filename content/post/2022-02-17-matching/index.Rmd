---
title: 'A guide to Matching, in R and with DAGs - Part 1'
author: Francisco Yirá
date: '2022-02-20'
slug: matching
isMath: "true"
categories:
  - causal-inference
  - data-science
  - R
tags:
  - books
  - causal-inference-the-mixtape
  - the-effect
  - english-only-ds
  - dags
  - summaries
  - matching
---

Until this moment, the posts about causal inference on this blog have been centred around conceptual frameworks that enable the discussion of causal inference problems, such as Directed Acyclical Graphs and the Potential Outcomes model[^1]. Now it's time to go one step further and start talking about the "toolbox" that allows us to address causal inference questions when working with **observational data** (that is, data where the treatment variable is not under the full control of the researcher).

[^1]: This blog post builds on top of the concepts introduced on those articles, so if you feel that there a lot of unkown words in this post, it may be helpful to check those previous posts first.

We've already seen that in randomised experiments it's relatively easy to obtain unbiased estimates of causal effects because the **independence assumption holds**. If the data scientist has assigned the treatment variable in a way that has nothing to do with the potential outcomes of the units, then the simple difference in outcomes (SDO) between the control and treated group is an unbiased estimator of the average treatment effect (ATE).

But there are a lot of situations when is not possible to carry out a randomised experiment. If this is the case, it's *almost certain* that the independence assumption doesn't hold[^2].

[^2]: *Especially* if we have free agents deciding their treatment status on their own.

Even worse, we could have experimental data and still not meet the independence assumption. How is that? Well, the treatment could be randomised within subgroups of the population (e.g. children grouped in schools or customers grouped in tiers or regions) but having different treatment probabilities across the subgroups. If these subgroups have systematic differences in potential outcomes, then there will be correlation between the potential outcomes and the treatment variable, and the independence assumption won't hold.

But wait, we still have a way out in situations like that. We may leverage the **conditional independence assumption**.

## Conditional Independence

In most cases, expecting your data to meet $Y^1,Y^0 \perp D$ is asking for too much. A more realistic aspiration is to have $Y^1,Y^0 \perp D |X$, the cheap, more common version of the independence assumption, known as **conditional independence**.

What conditional independence says is that $D$ could be correlated with potential outcomes in the data as a whole (no bueno), but if we look at subsets of observations with the same values of some covariates X (e.g. customers in the same tier or region, or children in the same school), then we have independence between $D$ and $Y^1,Y^0$ inside those subsets (good!).

(mom, can we have X at home meme\])

If the conditional independence assumption (CIA) is meet, then we can estimate the ATE using something known as the **subclassification estimator**. In plain English, all that this estimator does is calculating the SDO in each subgroup with the same values of X (strata) and then computing a **weighted average** of those SDOs using the frequency count of each strata as weight. Thus, if some strata has a lot of observations, its SDO is going to have a bigger influence on the final estimate.

Let's see a code example to understand this better. First, I'm going to declare the estimator as an R function:

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(rlang)

# Declaring the Subclassification Estimator as an R function
subclas_estimator <- function(df, outcome = Y, treatment = D, covariates) {
    df %>% 
    # we create one strata for each set of values of X present in the data
    group_nest({{ covariates }}) %>% 
    # Then, we compute a frequency count and an SDO for each strata
    mutate(freq = map_dbl(data, nrow),
           strata_sdo = map_dbl(data, sdo, {{ outcome }}, {{ treatment }})) %>% 
    # The estimate is a weighted average of the strata-specific SDOs 
    # using the frequency counts as weights
    summarise(estimate_ATE = weighted.mean(strata_sdo, freq)) %>% 
    pull(estimate_ATE)

}

# Auxiliary function that computes the SDO
sdo <- function(df, outcome = Y, treatment = D) {
  
  df <- df %>% 
    group_by({{treatment}}) %>% 
    summarise(mean = mean({{outcome}}))
  
  mean_t <- 
    df %>% 
    filter({{treatment}} == 1) %>% 
    pull(mean)
  
  mean_u <- 
    df %>% 
    filter({{treatment}} == 0) %>% 
    pull(mean)
  
  mean_t - mean_u
  
}
```

Now let's simulate a dataset where we have **conditional** independence (but not *un*conditional independence) and see the differences that emerge between the SDO and the subclassification estimator (i.e. the bias in the estimation due to not adjusting by X).

Just for fun, I'll give some back-story to this simulated data. You work as a salesperson at the paper company Dunder Muffin, and one day you suggest your boss, Miguel Scott, doing an experiment to evaluate how much sales would increase if we given a promotional discount to your customers in this time of the year. Miguel likes your idea, but he requests you to have a smaller control group (i.e. giving the discount to more customers) in the regions that are having lower sales. He is assuming that the treatment will have a positive effect, and therefore the purpose of the experiment would be just to get an estimate of *how much* the sales will increase in each region.

+------------+---------------------+-------------------------------------------------------------------+------------------------------+
| Region     | Number of customers | Average monthly sales per customer (USD) previous to the discount | Proportion Treated / Control |
+============+=====================+===================================================================+==============================+
| East       | 130                 | 3,000                                                             | 50% / 50%                    |
+------------+---------------------+-------------------------------------------------------------------+------------------------------+
| West       | 220                 | 2,000                                                             | 80% / 20%                    |
+------------+---------------------+-------------------------------------------------------------------+------------------------------+
| North      | 500                 | 3,500                                                             | 20% / 80%                    |
+------------+---------------------+-------------------------------------------------------------------+------------------------------+

For this part, we will assume an homogeneous treatment effect across regions: a 300 USD average increase on monthly sales per customer, even after taking the discount into account.

As you may already suspect, under this conditions the SDO will be biased, because the treatment assignment is correlated with the level of sales (the lower the monthly sales, the higher the chance of having $D=1$, that is, receiving the discount).

```{r}
set.seed(1989)

n_east <- 130
n_west <- 220
n_north <- 500
n_total <- n_east+n_west+n_north

customers <-
  tibble(
    customer_id = seq(n_total),
    region = c(rep("East", n_east),
               rep("West", n_west),
               rep("North", n_north)),
    # Treatment effect is 300USD + random component
    treatment_effect = rnorm(n_total, mean = 300, sd = 200),
    # Sales per customer when no discount is offered (untreated)
    y0 = c(
      rnorm(n_east, mean = 3000, sd = 200),
      rnorm(n_west, mean = 2000, sd = 200),
      rnorm(n_north, mean = 3500, sd = 200)
    ),
    # Sales per customer when discount is offered (treated)
    y1 = y0 + treatment_effect,
    # Treatment status
    d = c(
      rbinom(n_east, 1, 0.5),
      rbinom(n_west, 1, 0.8),
      rbinom(n_north, 1, 0.2)
    ),
    # Switching equation
    y = y0 + treatment_effect*d
    
  )
```

Obtaining the `sdo` over the whole customer base:

```{r}
sdo(customers, outcome = y, treatment = d)
```

And the `subclas_estimator` using `region` as covariate.

```{r}
subclas_estimator(customers,
                  outcome = y,
                  treatment = d,
                  covariates = region)
```

Because we have conditional independence after controlling by `region`, the `subclas_estimator` returns an estimate very close to the real treatment effect. Meanwhile, the `sdo` on the whole data has a huge negative bias, due to the correlation between the treatment assignment and the potential outcomes (lack of *un*conditional independence).

We can see this correlation more clearly in the following visualisations:

```{r}
customers_4_plot <- customers %>% 
  select(region, d, y1, y0) %>% 
  mutate(d = factor(d, levels = c(0,1),
                    labels = c("Untreated", "Treated"))) %>% 
  pivot_longer(cols = c(y1, y0),
               names_to = "potential_outcome",
               values_to = "value")

ggplot(customers_4_plot) +
  aes(potential_outcome, value, colour = factor(d)) +
  geom_boxplot() +
  labs(x = "Potential Outcome",
       y = "Monthly sales per customer",
       colour = "Treatment status",
       title = "There is no unconditional independence in this data",
       subtitle = "Potential outcomes are correlated with the treatment status")
```

```{r}
ggplot(customers_4_plot) +
  aes(potential_outcome, value, colour = factor(d)) +
  geom_boxplot() +
  facet_wrap(~region) +
  labs(x = "Potential Outcome",
       y = "Monthly sales per customer",
       colour = "Treatment status",
       title = "But there is conditional independence",
       subtitle = str_wrap("After controlling by X (Region), potential outcomes are no longer correlated with the treatment status"))
```

Note that this kind of visualisations are impossible to do with real world data, since counterfactual potential outcomes are unknown. In this example, we were able to look at them only because the data was simulated / created by ourselves.

This leads us to the following questions:

### When do we have conditional independence? And what variables should we include in X?

The best way to answer both questions is to draw a DAG (or causal diagram) of the relationships between the variables in our data. This diagram constitutes the best representation of all the knowledge we have about the data generating process.

A convenient way of drawing the DAG is to use the `ggdag` package in R.

For our Dunder Muffin example, the DAG and code would be as follows:

```{r, message=FALSE, warning=FALSE, scale=0.5}
library(ggdag)

# The `dagify` function allows to declare the DAG as a set of R formulas,
# plus a couple arguments to specify the outcome and treatment variables
dag <- dagify(sales ~ discount + region,
              discount ~ region,
              exposure = "discount",
              outcome = "sales") %>%
  # This transforms the output of `dagify` into a tidy dataframe
  tidy_dagitty()

# And now we use the dataframe to create a ggplot with dag geoms
dag %>% 
 ggplot(aes(
    x = x,
    y = y,
    xend = xend,
    yend = yend
  )) +
  geom_dag_point(colour = "deepskyblue3", size = 15) +
  geom_dag_edges() +
  geom_dag_label_repel(aes(label = name)) +
  theme_dag()
```

This particular DAG tells us that:

-   `sales` (the outcome variable) is affected by both `region` and `discount` (the treatment variable)

-   `discount` is affected by `region`: there is a different chance of receiving the treatment based on the region where the customer is located.

The last relationship is the one that kills the independence assumption by opening a backdoor path between the treatment and the outcome:

`sales` ⬅️`region` ➡️`discount`

In this backdoor path, `region` acts as a confounder that creates spurious correlation between the treatment and the outcome.

```{r}
confounder_triangle(x = "sales", y = "discount", z = "region") %>% 
  ggdag_dconnected(text = FALSE, use_labels = "label") +
  theme_dag()
```

Fortunately, the confounder variable is *observable*: we know its values for each observation. Thus, we can condition on it and block the backdoor path. By doing this, we isolate the true causal effect of `discount` on `sales`.

This is what the `subclas_estimator` did, and it's the reason why it returned an appropriate estimate when the SDO was biased.

So, ***when do we have conditional independence?***

When it's possible to close all the backdoors between the treatment and outcome variable by conditioning on a set of observable variables (*X*).

***And what variables should we condition on?***

Well, the ones that allows us to close the backdoors!

**If there are backdoor paths that would require to condition on *un*observable variables to be closed, then we don't have conditional independence anymore (because we can't condition on variables that are not available in our data!).**

If we have an accurate DAG, we can use the function `ggdag_adjustment_set` from `ggdag` to discover what sets of variables (if any) allow us to meet the conditional independence assumption. Pretty handy.

```{r}
ggdag_adjustment_set(dag,
                     text = FALSE, use_labels = "name", shadow = TRUE) +
  theme_dag()

```

### Girls just want to have ~~fun~~ an accurate DAG

What we saw above highlights the importance of having an accurate DAG, since it will tell us if we can achieve conditional independence or not.

Here someone may ask where does this magical accurate DAG come from. As it was said before, it is a representation of our current knowledge about the problem. This knowledge could be flawed, and then the DAG would be flawed too.

Unfortunately, there is no workaround to this conundrum. In order to do causal inference, we must have a clear understanding of the data generating process. **And whether we draw the DAG or not, we're assuming an underlying DAG anyway (a set of relationships between the variables)**.

If you go ahead and use the `subclas_estmator`[^3], then you're assuming that you have conditional independence after adjusting by X. So, better to make this assumptions explicit in a DAG and then look at it and asks ourselves if it's a good representation of the data generating process or if we are missing something[^4].

[^3]: Or multiple linear regression with a set of control variables.

[^4]: All this is explained with greater detail in a previous blog post about DAGs. If these paragraphs sound weird, then it's a good idea to pause and read that post first.

The good news is that whenever you have experimental data (i.e. when *you* decide the treatment assignment) drawing the DAG is relatively simple: you know exactly what is causing $D$, so you can easily adjust for it if it's a confounder (i.e. if it also affects $Y$). In our Dunder Muffin example, we are 100% sure that the treatment depends on `region` and nothing else, so closing the backdoor is a walk in the park[^5].

[^5]: The part that may be tricky in the real world is to know whether the variables that cause $D$ are also linked with $Y$. In our example, we know `region` affects `sales` because the data itself was created by us, but the relationships between $X$ and $Y$ are going to be assumptions most of the time.

On the other hand, if your data is *observational*, then drawing the DAG and arguing that the CIA is meet is a much more challenging task. In this case you may want to take more time to learn everything you can about how the data is generated[^6] and how the variables relate to each other (*especially* the variables linked to the outcome or the treatment)[^7].

[^6]: If you work as a data scientist in a company, get up from your desk, roll up your sleeves and ask questions to those involved in the data collection or data generation itself.

[^7]: Seriously, this is ***extremely important***. If you can't plausibly argue about having conditional independence in your data, then none of the estimators described in this post will help you to isolate the relevant causal effect.

## The Exact Matching Estimator

Now let's move on to a more advanced (and more commonly used) version of the `subclas_estimator`: the **Exact Matching Estimator**. This estimator does in principle the same thing as the `subclas_estimator`, it splits the dataset based on the values of the confounders X and then looks at the differences between treatment and control units within each sub-group.

However, instead of calculating the `sdo` in each sub-group and then weighting it by frequency, it tries to **impute the counterfactual value for *each observation* by matching them to another observation that has the same values in the variables X, but opposite treatment status**. Then, it computes the difference in outcome for each pair unit-counterfactual and take the average of those differences.

In real-world analysis, exact matching often comes accompanied with *coarsening* or *binning* of continuous variables in X (in which case is regarded as **coarsened exact matching** or CEM). The reason for this is that you will usually have continuous variables for which is impossible to find pairs of units with exactly the same value, even if the dataset is very large. So, you have to do a sensible discretisation (e.g. creating bins such as 0 to 2, 2 to 4, 4 to 6, and so on) and then match based on the categorical values.

Let's look at this exact matching estimator defined as an R function[^8]:

[^8]: I will left the *coarsening* part out of this because it can be regarded as pre-processing you have to do on the data inputs in a case-by-case basis, and therefore can not be abstracted away in a general function. The function itself will assume that the `covariates` are categorical (in the general sense, not necessarily R factors).

```{r}
# Extact Matching Estimator of the ATT
exact_matching_estimator_att <- function(df, #dataframe
                                         outcome, # variable name
                                         treatment, # variable name
                                         covariates # string or vector of strings
                                         ) {
  
   # Renaming the outcome column internally for convenience
  df <- df %>%
    rename(outcome := {{ outcome }}) %>% 
    mutate(id = 1:n())
  
  # Creating a dataset that contain only the treated units
  treated_df <- df %>% 
    filter({{ treatment }} == 1)
  
  # And another only with the untreated units
  control_df <- df %>% 
    filter({{ treatment }} == 0)
  
  # Matching the treated to untreated units with
  # equal covariate values, thus creating
  # a comparable control sample
  treated_matched <- treated_df %>% 
    left_join(control_df, by = covariates,
              suffix = c("_i", "_j"))
  
  estimate_att <- 
    treated_matched %>% 
    # averaging the outcomes of the matches if there are several
    group_by(id_i) %>%
    summarise(outcome_j = mean(outcome_j),
              outcome_i = first(outcome_i)) %>%
    # calculating differences between
    # outcome of each unit and 
    # the average outcome of their matches
    mutate(treat_effect = outcome_i - outcome_j) %>% 
    # averaging those differences
    summarise(estimate_att = mean(treat_effect)) %>% 
    # voila
    pull(estimate_att)

  estimate_att
}

```

Now let's try this estimator on a dataset (this one comes from the book [Causal Inference for the Brave and True](https://matheusfacure.github.io/python-causality-handbook/10-Matching.html#matching-estimator)):

```{r, message=FALSE, warning=FALSE}
trainees_df <- read_csv("https://github.com/matheusfacure/python-causality-handbook/raw/master/causal-inference-for-the-brave-and-true/data/trainees.csv",
                        show_col_types=FALSE) %>% 
  rename(trainee = trainees)

trainees_df

```

Here the treatment is a `trainee` program and each observation is an employee who may have taken the program or not. `earnings` is the outcome variable, and `age` is a covariate that turns out to be a confounder, since it affects both `earnings` and the likelihood to have taken the treatment: younger people are more likely to enrol in the program, but also have lower `earnings`, because they have less experience.

```{r}
library(ggside)

trainees_df %>% 
  mutate(treatment_status = factor(trainee,
                          levels = c(0, 1),
                          labels = c("Trainee", "Not Trainee"))) %>% 
ggplot() +
  aes(age, earnings, color = treatment_status) +
  geom_jitter() +
  geom_xsidedensity(
    aes(
      y = after_stat(density),
      fill = treatment_status
    ),
    alpha = 0.5,
    size = 1,
    position = "identity"
  ) +
  geom_ysidedensity(
    aes(
      x = after_stat(density),
      fill = treatment_status
    ),
    alpha = 0.5,
    size = 1,
    position = "identity"
  ) + 
  labs(title = "Trainees tend to be younger AND to have lower earnings") +
  theme(
    ggside.panel.scale.x = 0.3,
    ggside.panel.scale.y = 0.3,
    ggside.axis.text.y = element_blank(),
    ggside.axis.ticks.y = element_blank(),
    ggside.axis.text.x = element_blank(),
    ggside.axis.ticks.x = element_blank()
  )
```

Thus, the problem here is self-selection, not randomisation conditional on some variable. But we have conditional independence anyway (after adjusting by `age`):

```{r}
dag_trainees <- dagify(earnings ~ age + trainee,
                       trainee ~ age,
                       exposure = "trainee",
                       outcome = "earnings")

ggdag_adjustment_set(dag_trainees,
                     text = FALSE,
                     use_labels = "name",
                     shadow = TRUE) +
  theme_dag()
```

Great. Now it's time to use the `exact_matching_estimator_att` on this data:

```{r}
exact_matching_estimator_att(
  trainees_df,
  outcome = earnings,
  treatment = trainee,
  covariates = "age"
)
```

As we see, once we adjust by `age` using exact matching, we get a large and positive treatment effect estimate. But, unlike in the `subclas_estimator`, this new function is estimating the treatment effect *on the treated only*. You can tell this by looking at its source code above, where only the treated units are matched with a control unit, but the control units are not matched with comparable treated units. Therefore, it's an ATT estimator.

Let's see how an ATE exact matching estimator would look like (here I'm commenting only the parts that change compared to the ATT estimator).

```{r}
# Extact Matching Estimator of the ATE
exact_matching_estimator_ate <- function(df, #dataframe
                                         outcome, # variable name
                                         treatment, # variable name
                                         covariates # string or vector of strings
                                         ) {
  df <- df %>%
    rename(outcome := {{ outcome }},
           # renaming treatment column for convenience
           treatment := {{ treatment }}) %>% 
    mutate(id = 1:n())
  
  treated_df <- df %>% 
    filter(treatment == 1)
  
  control_df <- df %>% 
    filter(treatment == 0)
  
  treated_matched <- treated_df %>% 
    left_join(control_df, by = covariates,
              suffix = c("_i", "_j"))
  
  # Now we also match the control units to treated units 
  # with the same covariates values
  control_matched <- control_df %>% 
    left_join(treated_df, by = covariates,
              suffix = c("_i", "_j"))
  
  estimate_ate <- 
    # We merge the two matched samples
    bind_rows(
      treated_matched,
      control_matched
    ) %>% 
    group_by(id_i) %>%
    summarise(outcome_j = mean(outcome_j),
              outcome_i = first(outcome_i),
              treatment_i = first(treatment_i)) %>%
    mutate(treat_effect = outcome_i - outcome_j,
           # We multiply by -1 the differences between controls
           # and their matches. This makes sense since those 
           # differences are like an "anti treatment effect"
           treat_effect = ifelse(
             treatment_i == 1,
             yes = treat_effect,
             no = -1*treat_effect
           )) %>%
    summarise(estimate_ate = mean(treat_effect)) %>% 
    pull(estimate_ate)

  estimate_ate
}
```

The key difference between this ATE estimator and the previous ATT estimator is that here we also create a `control_matched` sample with treated units that are regarded as counterfactuals of the true control units (because they have the same `covariates` values).

Then we `bind_rows()` both matched samples and compute the `mean` of the differences between actual and counterfactual outcomes as before, BUT first multiplying by -1 the differences between controls and their matches. We do this because, if you think about it, the difference between a control and its counterfactual can be seen as a "anti-treatment effect": if the treatment effect is positive, we expect this difference to be negative.

Now let's try this `exact_matching_estimator_ate` on our `trainee_df`:

```{r}
exact_matching_estimator_ate(
  trainees_df,
  outcome = earnings,
  treatment = trainee,
  covariates = "age"
)
```

We got `NA`. What happened? I'll do the matching manually to check what went wrong:

```{r}
treated_df <- trainees_df %>%
  filter(trainee == 1)

control_df <- trainees_df %>%
  filter(trainee == 0)

treated_matched <- treated_df %>%
  left_join(control_df, by = "age",
                     suffix = c("_i", "_j"))

control_matched <- control_df %>%
  left_join(treated_df, by = "age",
            suffix = c("_i", "_j"))
    
bind_rows(
  control_matched,
  treated_matched
)
```

We see several `NA`s in the columns with suffix `_j`, that is, the columns that should contain the values coming from the matches. Since the only change versus the previous estimator is that now we try to match the control units, the problems seems to be that **the "donor pool" of treated units doesn't contain exact matches for each control unit. Therefore, we can't estimate the ATE with** `exact_matching_estimator_ate`**.**

How unfortunate! But also convenient, because this helps to introduce a very important topic: the **common support assumption** and **the curse of dimensionality**.

## The Common Support Assumption

Exact matching is a powerful and useful method, as long as two key assumptions are met. The first one is the CIA (conditional independence assumption), which we already discussed in detail. If the CIA is not met, the estimate we get through exact matching will be biased, because there will still be backdoors open in our DAG.

The second one is the **common support assumption**. It means that there should be both control and treated observations for each combination of values of X that exists in the data. If this assumption is not met, we will end up with units without matches, and the estimator for the ATE cannot be calculated (as it happened with `exact_matching_estimator_ate` and the `trainee_df`).

One way to approach the lack of common support is to **just give up with estimating the ATE and focus on the ATT instead**. When estimating the ATT, we still need to have control units for all the values of X that have treated units, but not vice versa, so we're "lowering the bar". And if our treated group is small compared to the whole dataset, then it's much more likely to have common support to estimate the ATT. In fact, we already saw an example of this with the `trainee_df`, where it was possible to estimate the ATT (but not the ATE) with exact matching.

But what if there is not even common support for the treated units? Well, then exact matching can not possible estimate the ATT. Still, it could provide a treatment effect estimate for "those units who got matched" (for example, in `exact_matching_estimator_ate` we could have dropped the rows without matches and have gotten a result anyway), but since this is not an estimate of the ATT or the ATE (the averages we are usually interested in), this figure probably won't be helpful to our audience or stakeholders.

And now the time has come to meet the sinister archrival of the matching methodologies: **the curse of dimensionality**.

## The Curse of Dimensionality

This is a phenomena that occurs whenever we work with a lot of variables (a.k.a. dimensions). If you come from the machine learning world you may already be familiar with this problem, but for those who don't, I'm going to drop the [Wikipedia definition](https://en.wikipedia.org/wiki/Curse_of_dimensionality):

> when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. In order to obtain a reliable result, the amount of data needed often grows exponentially with the dimensionality

For our matching estimators this means that as more variables are considered in the covariate matrix X, the likelihood of finding exact matches[^9] for all units quickly approaches zero. Put another way, this curse **reduces our chances of having common support**.

[^9]: Or even approximate matches! As we're going to see later.

![](images/matching_curse.jpg)

In fact, the curse of dimensionality is particularly cruel with the matching estimators. While in predictive machine learning we have some sort of trade off, with more features bringing in more information at the expense of increasing the distance in the feature space (i.e. we have a choice), in matching methodologies we theoretically have a "true" set X of confounders for which we are obliged to control / adjust for.

In other words, when doing matching it's not like we're using more covariates out of greed to have "as much information as we can"[^10], but instead, after carefully drawing our DAG we should end up with a list of variables for which we **must** control for in order to close the backdoors. We shouldn't use more variables, and we shouldn't use less.

[^10]: Well, some people do that, but we shouldn't.

If our confounding problem is simple, such as an experiment with randomisation conditional on some variable (like the Dunder Muffin example from above) then it's likely that we will only need a few covariates, or even just one, and thus the curse will be avoided.

But if we end up needing more, we're in trouble.

The conclusions above reveal some problems with our until now beloved `subclas_estimator`. In the toy example we studied there was, very conveniently, only one observable confounder: `region`. Moreover, this confounder was categorical and it divided the data in relatively big strata: the smallest `region` had 130 customers.

But what if we found out that we have to adjust for a lot of variables? And what if some of those variables are continuous? We could address the continuous variable problem by discretising it in bins, but we could still end up with a lot of strata, and some of them could have no observations in either the treated or control group. If that is the case, the `subclas_estimator` will become undefined (return `NA`) because the `sdo` cannot be calculated for subgroups that don't have both control and treated observatiions.

This requirement (having observations of both groups in each stratum) is known as the **common support assumption**. And it becomes increasingly difficult to meet this assumption as the number of covariates X increases.

The reason for this is **the curse of dimensionality**,

## The (Approximate) Matching Estimator

(skip directly to the approximate matching estimator)

(explain that there a lot of different parameters to decide when we do matching... but here we're going to use a specifical

## Differences with Regression

If the CIA is meet in our data, we could just use regression.

```{r}
lm(y ~ d + region, data = customers) %>% 
  summary()
```

## Distance Matching

## Propensity Scores

### Don't treat propensity scores as a machine learning prediction problem

## Double Robust Estimator

## ATE vs ATT vs ATU: How to get each one with matching?
