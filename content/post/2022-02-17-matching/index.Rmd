---
title: 'A brief field guide to Matching (in R and with DAGs) - Part 1'
author: Francisco Yir√°
date: '2022-02-20'
slug: matching
isMath: "true"
categories:
  - causal-inference
  - data-science
  - R
tags:
  - books
  - causal-inference-the-mixtape
  - the-effect
  - english-only-ds
  - dags
  - summaries
  - matching
---

Until this moment, the posts about causal inference on this blog have been centred around conceptual frameworks that enable the discussion of causal inference problems, such as Directed Acyclical Graphs and the Potential Outcomes model[^1]. Now it's time to go one step further and start talking about the "toolbox" that allows us to address causal inference questions when working with **observational data** (that is, data where the treatment variable is not under the full control of the researcher).

[^1]: This blog post builds on top of the concepts introduced on those articles, so if you feel that there a lot of unkown words in this post, it may be helpful to check those previous posts first.

We've already seen that in randomised experiments it's relatively easy to obtain unbiased estimates of causal effects because the **independence assumption holds**. If the data scientist has assigned the treatment variable in a way that has nothing to do with the potential outcomes of the units, then the simple difference in outcomes (SDO) between the control and treated group is an unbiased estimator of the average treatment effect (ATE).

But there are a lot of situations when is not possible to carry out a randomised experiment. If this is the case, it's *almost certain* that the independence assumption doesn't hold[^2].

[^2]: *Especially* if we have free agents deciding their treatment status on their own.

Even worse, we could have experimental data and still not meet the independence assumption. How is that? Well, the treatment could be randomised within subgroups of the population (e.g. children grouped in schools or customers grouped in tiers or regions) but having different treatment probabilities across the subgroups. If these subgroups have systematic differences in potential outcomes, then there will be correlation between the potential outcomes and the treatment variable, and the independence assumption won't hold.

But wait, we still have a way out in situations like that. We may leverage the **conditional independence assumption**.

## Conditional Independence

In most cases, expecting your data to meet $Y^1,Y^0 \perp D$ is asking for too much. A more realistic aspiration is to have $Y^1,Y^0 \perp D |X$, the cheap, more common version of the independence assumption, known as **conditional independence**.

What conditional independence says is that $D$ could be correlated with potential outcomes in the data as a whole (no bueno), but if we look at subsets of observations with the same values of some covariates X (e.g. customers in the same tier or region, or children in the same school), then we have independence between $D$ and $Y^1,Y^0$ inside those subsets (good!).

(mom, can we have X at home meme\])

If the conditional independence assumption (CIA) is meet, then we can estimate the ATE using something known as the **subclassification estimator**. In plain English, all that this estimator does is calculating the SDO in each subgroup with the same values of X (strata) and then computing a **weighted average** of those SDOs using the frequency count of each strata as weight. Thus, if some strata has a lot of observations, its SDO is going to have a bigger influence on the final estimate.

Let's see a code example to understand this better. First, I'm going to declare the estimator as an R function:

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(rlang)

# Declaring the Subclassification Estimator as an R function
subclas_estimator <- function(df, outcome = Y, treatment = D, covariates) {
    df %>% 
    # we create one strata for each set of values of X present in the data
    group_nest({{ covariates }}) %>% 
    # Then, we compute a frequency count and an SDO for each strata
    mutate(freq = map_dbl(data, nrow),
           strata_sdo = map_dbl(data, sdo, {{ outcome }}, {{ treatment }})) %>% 
    # The estimate is a weighted average of the strata-specific SDOs 
    # using the frequency counts as weights
    summarise(estimate_ATE = weighted.mean(strata_sdo, freq)) %>% 
    pull(estimate_ATE)

}

# Auxiliary function that computes the SDO
sdo <- function(df, outcome = Y, treatment = D) {
  
  df <- df %>% 
    group_by({{treatment}}) %>% 
    summarise(mean = mean({{outcome}}))
  
  mean_t <- 
    df %>% 
    filter({{treatment}} == 1) %>% 
    pull(mean)
  
  mean_u <- 
    df %>% 
    filter({{treatment}} == 0) %>% 
    pull(mean)
  
  mean_t - mean_u
  
}
```

Now let's simulate a dataset where we have **conditional** independence (but not *un*conditional independence) and see the differences that emerge between the SDO and the subclassification estimator (i.e. the bias in the estimation due to not adjusting by X).

Just for fun, I'll give some back-story to this simulated data. You work as a salesperson at the paper company Dunder Muffin, and one day you suggest your boss, Miguel Scott, doing an experiment to evaluate how much sales would increase if we given a promotional discount to your customers in this time of the year. Miguel likes your idea, but he requests you to have a smaller control group (i.e. giving the discount to more customers) in the regions that are having lower sales. He is assuming that the treatment will have a positive effect, and therefore the purpose of the experiment would be just to get an estimate of *how much* the sales will increase in each region.

+------------+---------------------+-------------------------------------------------------------------+------------------------------+
| Region     | Number of customers | Average monthly sales per customer (USD) previous to the discount | Proportion Treated / Control |
+============+=====================+===================================================================+==============================+
| East       | 130                 | 3,000                                                             | 50% / 50%                    |
+------------+---------------------+-------------------------------------------------------------------+------------------------------+
| West       | 220                 | 2,000                                                             | 80% / 20%                    |
+------------+---------------------+-------------------------------------------------------------------+------------------------------+
| North      | 500                 | 3,500                                                             | 20% / 80%                    |
+------------+---------------------+-------------------------------------------------------------------+------------------------------+


For this part, we will assume an homogeneous treatment effect across regions: a 300 USD average increase on monthly sales per customer, even after taking the discount into account.

As you may already suspect, under this conditions the SDO will be biased, because the treatment assignment is correlated with the level of sales (the lower the monthly sales, the higher the chance of having $D=1$, that is, receiving the discount).

```{r}
set.seed(1989)

n_east <- 130
n_west <- 220
n_north <- 500
n_total <- n_east+n_west+n_north

customers <-
  tibble(
    customer_id = seq(n_total),
    region = c(rep("East", n_east),
               rep("West", n_west),
               rep("North", n_north)),
    # Treatment effect is 300USD + random component
    treatment_effect = rnorm(n_total, mean = 300, sd = 200),
    # Sales per customer when no discount is offered (untreated)
    y0 = c(
      rnorm(n_east, mean = 3000, sd = 200),
      rnorm(n_west, mean = 2000, sd = 200),
      rnorm(n_north, mean = 3500, sd = 200)
    ),
    # Sales per customer when discount is offered (treated)
    y1 = y0 + treatment_effect,
    # Treatment status
    d = c(
      rbinom(n_east, 1, 0.5),
      rbinom(n_west, 1, 0.8),
      rbinom(n_north, 1, 0.2)
    ),
    # Switching equation
    y = y0 + treatment_effect*d
    
  )
```

Obtaining the `sdo` over the whole customer base:

```{r}
sdo(customers, outcome = y, treatment = d)
```

And the `subclas_estimator` using `region` as covariate.

```{r}
subclas_estimator(customers,
                  outcome = y,
                  treatment = d,
                  covariates = region)
```

Because we have conditional independence after controlling by `region`, the `subclas_estimator` returns an estimate very close to the real treatment effect. Meanwhile, the `sdo` on the whole data has a huge negative bias, due to the correlation between the treatment assignment and the potential outcomes (lack of *un*conditional independence).

We can see this correlation more clearly in the following visualisations:

```{r}
customers_4_plot <- customers %>% 
  select(region, d, y1, y0) %>% 
  mutate(d = factor(d, levels = c(0,1),
                    labels = c("Untreated", "Treated"))) %>% 
  pivot_longer(cols = c(y1, y0),
               names_to = "potential_outcome",
               values_to = "value")

ggplot(customers_4_plot) +
  aes(potential_outcome, value, colour = factor(d)) +
  geom_boxplot() +
  labs(x = "Potential Outcome",
       y = "Monthly sales per customer",
       colour = "Treatment status",
       title = "There is no unconditional independence in this data",
       subtitle = "Potential outcomes are correlated with the treatment status")
```

```{r}
ggplot(customers_4_plot) +
  aes(potential_outcome, value, colour = factor(d)) +
  geom_boxplot() +
  facet_wrap(~region) +
  labs(x = "Potential Outcome",
       y = "Monthly sales per customer",
       colour = "Treatment status",
       title = "But there is conditional independence",
       subtitle = str_wrap("After controlling by X (Region), potential outcomes are no longer correlated with the treatment status"))
```

Note that this kind of visualisations are impossible to do with real world data, since counterfactual potential outcomes are unknown. In this example, we were able to look at them only because the data was simulated / created by ourselves.

This leads us to the following questions:

### When do we have conditional independence? And what variables should we include in X?

The best way to answer both questions is to draw a DAG (or causal diagram) of the relationships between the variables in our data. This diagram constitutes the best representation of all the knowledge we have about the data generating process.

A convenient way of drawing the DAG is to use the `ggdag` package in R.

For our Dunder Muffin example, the DAG and code would be as follows:

```{r, message=FALSE, warning=FALSE, scale=0.5}
library(ggdag)

# The `dagify` function allows to declare the DAG as a set of R formulas,
# plus a couple arguments to specify the outcome and treatment variables
dag <- dagify(sales ~ discount + region,
              discount ~ region,
              exposure = "discount",
              outcome = "sales") %>%
  # This transforms the output of `dagify` into a tidy dataframe
  tidy_dagitty()

# And now we use the dataframe to create a ggplot with dag geoms
dag %>% 
 ggplot(aes(
    x = x,
    y = y,
    xend = xend,
    yend = yend
  )) +
  geom_dag_point(colour = "deepskyblue3", size = 15) +
  geom_dag_edges() +
  geom_dag_label_repel(aes(label = name)) +
  theme_dag()
```

This particular DAG tells us that:

-   `sales` (the outcome variable) is affected by both `region` and `discount` (the treatment variable)

-   `discount` is affected by `region`: there is a different chance of receiving the treatment based on the region where the customer is located.

The last relationship is the one that kills the independence assumption by opening a backdoor path between the treatment and the outcome:

`sales` ‚¨ÖÔ∏è`region` ‚û°Ô∏è`discount`

In this backdoor path, `region` acts as a confounder that creates spurious correlation between the treatment and the outcome.


```{r}
confounder_triangle(x = "sales", y = "discount", z = "region") %>% 
  ggdag_dconnected(text = FALSE, use_labels = "label") +
  theme_dag()
```

Fortunately, the confounder variable is *observable*: we know its values for each observation. Thus, we can condition on it and block the backdoor path. By doing this, we isolate the true causal effect of `discount` on `sales`.

This is what the `subclas_estimator` did, and it's the reason why it returned an appropriate estimate when the SDO was biased.

So, ***when do we have conditional independence?***

When it's possible to close all the backdoors between the treatment and outcome variable by conditioning on a set of observable variables (*X*).


***And what variables should we condition on?***

Well, the ones that allows us to close the backdoors!


**If there are backdoor paths that would require to condition on *un*observable variables to be closed, then we don't have conditional independence anymore (because we can't condition on variables that are not available in our data!).**

If we have an accurate DAG, we can use the function `ggdag_adjustment_set` from `ggdag` to discover what sets of variables (if any) allow us to meet the conditional independence assumption. Pretty handy.


```{r}
ggdag_adjustment_set(dag,
                     text = FALSE, use_labels = "name", shadow = TRUE) +
  theme_dag()

```


### Girls just want to have ~~fun~~ an accurate DAG

What we saw above highlights the importance of having an accurate DAG, since it will tell us if we can achieve conditional independence or not.

Here someone may ask where does this magical accurate DAG come from. As it was said before, it is a representation of our current knowledge about the problem. This knowledge could be flawed, and then the DAG would be flawed too.

Unfortunately, there is no workaround to this conundrum. In order to do causal inference, we must have a clear understanding of the data generating process. **And whether we draw the DAG or not, we're assuming an underlying DAG anyway (a set of relationships between the variables)**.

If you go ahead and use the `subclas_estmator`[^3], then you're assuming that you have conditional independence after adjusting by X. So, better to make this assumptions explicit in a DAG and then look at it and asks ourselves if it's a good representation of the data generating process or if we are missing something[^4].

[^3]: Or multiple linear regression with a set of control variables.

[^4]: All this is explained with greater detail in a previous blog post about DAGs. If these paragraphs sound weird, then it's a good idea to pause and read that post first.

The good news is that whenever you have experimental data (i.e. when *you* decide the treatment assignment) drawing the DAG is relatively simple: you know exactly what is causing $D$, so you can easily adjust for it if it's a confounder (i.e. if it also affects $Y$). In our Dunder Muffin example, we are 100% sure that the treatment depends on `region` and nothing else, so closing the backdoor is a walk in the park[^5].

[^5]: The part that may be tricky in the real world is to know whether the variables that cause $D$ are also linked with $Y$. In our example, we know `region` affects `sales` because the data itself was created by us, but the relationships between $X$ and $Y$ are going to be assumptions most of the time.

On the other hand, if your data is *observational*, then drawing the DAG and arguing that the CIA is meet is a much more challenging task. In this case you may want to take more time to learn everything you can about how the data is generated[^6] and how the variables relate to each other (*especially* the variables linked to the outcome or the treatment)[^7].

[^6]: If you work as a data scientist in a company, get up from your desk, roll up your sleeves and ask questions to those involved in the data collection or data generation itself.

[^7]: Seriously, this is ***extremely important***. If you can't plausibly argue about having conditional independence in your data, then none of the estimators described in this post will help you to isolate the relevant causal effect.

## The Exact Matching Estimator

Now let's move on to a more advanced (and more commonly used) version of the `subclas_estimator`: the **Exact Matching Estimator**. This estimator does in principle the same thing as the `subclas_estimator`, it splits the dataset based on the values of the confounders X and then looks at the differences between treatment and control units within each sub-group.

However, instead of calculating the `sdo` in each sub-group and then weighting it by frequency, it tries to **impute the counterfactual value for *each observation* by matching them to another observation that has the same values in the variables X, but opposite treatment status**. Then, it computes the difference in outcome for each pair unit-counterfactual and take the average of those differences.

In real-world analysis, exact matching often comes accompanied with *coarsening* or *binning* of continuous variables in X (in which case is regarded as **coarsened exact matching** or CEM). The reason for this is that you will usually have continuous variables for which is impossible to find pairs of units with exactly the same value, even if the dataset is very large. So, you have to do a sensible discretisation (e.g. creating bins such as 0 to 2, 2 to 4, 4 to 6, and so on) and then match based on the categorical values.

Let's look at this exact matching estimator defined as an R function[^8]:

[^8]: I will left the *coarsening* part out of this because it can be regarded as pre-processing you have to do on the data inputs in a case-by-case basis, and therefore can not be abstracted away in a general function. The function itself will assume that the `covariates` are categorical (in the general sense, not necessarily R factors).

```{r}
# Extact Matching Estimator of the ATT
exact_matching_estimator_att <- function(df, #dataframe
                                         outcome, # variable name
                                         treatment, # variable name
                                         covariates # string or vector of strings
                                         ) {
  
   # Renaming the outcome column internally for convenience
  df <- df %>%
    rename(outcome := {{ outcome }}) %>% 
    mutate(id = 1:n())
  
  # Creating a dataset that contain only the treated units
  treated_df <- df %>% 
    filter({{ treatment }} == 1)
  
  # And another only with the untreated units
  control_df <- df %>% 
    filter({{ treatment }} == 0)
  
  # Matching the treated to untreated units with
  # equal covariate values, thus creating
  # a comparable control sample
  treated_matched <- treated_df %>% 
    left_join(control_df, by = covariates,
              suffix = c("_i", "_j"))
  
  estimate_att <- 
    treated_matched %>% 
    # averaging the outcomes of the matches if there are several
    group_by(id) %>% 
    summarise(outcome_j = mean(outcome_j)) %>% 
    # calculating differences between
    # outcome of each unit and 
    # the average outcome of their matches
    mutate(treat_effect = outcome_i - outcome_j) %>% 
    # averaging those differences
    summarise(estimate_att = mean(treat_effect)) %>% 
    # voila
    pull(estimate_att)

  estimate_att
}

```

Now let's try this estimator on a dataset (this one comes from the book [Causal Inference for the Brave and True](https://matheusfacure.github.io/python-causality-handbook/10-Matching.html#matching-estimator)):

```{r, message=FALSE, warning=FALSE}
trainees_df <- read_csv("https://github.com/matheusfacure/python-causality-handbook/raw/master/causal-inference-for-the-brave-and-true/data/trainees.csv",
                        show_col_types=FALSE) %>% 
  rename(trainee = trainees)

trainees_df

```

Here the treatment is a `trainee` program and each observation is an employee who may have taken the program or not. `earnings` is the outcome variable, and `age` is a covariate that turns out to be a confounder, since it affects both `earnings` and the likelihood to have taken the treatment: younger people are more likely to enrol in the program, but also have lower `earnings`, because they have less experience.

Thus, the problem here is self-selection, not randomisation conditional on some variable. But we have conditional independence anyway (after adjusting by `age`):

```{r}
dag_trainees <- dagify(earnings ~ age + trainee,
                       trainee ~ age,
                       exposure = "trainee",
                       outcome = "earnings")

ggdag_adjustment_set(dag_trainees,
                     text = FALSE,
                     use_labels = "name",
                     shadow = TRUE) +
  theme_dag()
```

```{r}
ggplot(trainees_df) +
  aes(age, fill = factor(trainee)) + 
  geom_histogram(position = position_identity(),
                 alpha = 0.7) +
  labs(title = "Trainees tend to be younger and have lower earnings")
```

```{r}
ggplot(trainees_df) +
  aes(age, earnings, color = factor(trainee)) +
  geom_jitter() +
  # stat_density_2d(aes(alpha = ..piece..)) +
  # theme(plot.margin = margin())
  labs(title = "Trainees tend to be younger and have lower earnings")
```

Next: finish the plot

Obtain the estimate

Show the ATE estimator

Get an error

Move on to the common support assumption and the curse of dimensionality.

Another

-   Replacement

-   ATT or ATE? I think I'm going to estimate the ATE for now, and then "scale back" to ATT after explaining the curse of dimensionality.

And while the subclassification estimator left us little choice regarding how to implement it in code, the exact matching estimator is quite more flexible, in the sense that it allows for different variations depending on the answers to the following questions:

-   How many matches do we want to use for each unit?

-   Do we want to use matching *with* replacement or *without* replacement? That is, once a unit is matched to another, will it still be part of the "donor pool" of potential matches for the other observations?

-   If we use matching *without* replacement, in which order are we going to match the units? If we remove the units from the "donor pool" as they used as matches, then the order in which we match the units could affect the final estimate.

The answer to these questions has a lot to do with the **bias-variance trade-off**. If we do matching with replacement

Here we may ask what happens when there is more than one match for a given unit, or when one observation can be matched to several units. The answer is that we have several alternatives:

-   We could do matching *with replacement*. That is, after a control unit is matched to a treated unit, it is returned to the "donor pool" where it can be selected again to be matched to another treated unit with the same values of X.

-   Alternatively, we could do matching *without replacement*. Here the units are removed from the "donor pool" after being matched, so the units that come afterwards have to pick a match from a smaller donor pool.

Which option is better? It depends. Specifically, it depends on the **bias-variance trade-off**. Matching *with replacement* reduces bias by maximising the donor pool available to each of the observations to be matched (this maximises the chance of finding a match with the same values of X for each unit), whereas matching *without replacement* reduces variance by ensuring that you use each observation in the donor pool only once. This is important because it makes the final estimate less vulnerable to random variation in our particular sample or dataset (at the expense of limiting our chances of finding matches for all the units). If some observations are allowed to be matched to multiple units, they will have more leverage, which may be problematic if they turn to have extreme or non-representative values in the outcome variable.

TL;DR: you have to choose between having less bias (and thus pick matching with replacement) or less variance (and then choose matching without replacement).

Regarding what to do when a single unit has several potential matches, the most common solution is to take the average outcome of all the potential matches to impute the counterfactual for the original unit.

As you see, the exact matching estimator leaves room

## The Curse of Dimensionality

Now it's time to talk about the elephant in the room.

The conclusions above reveal some problems with our until now beloved `subclas_estimator`. In the toy example we studied there was, very conveniently, only one observable confounder: `region`. Moreover, this confounder was categorical and it divided the data in relatively big strata: the smallest `region` had 130 customers.

But what if we found out that we have to adjust for a lot of variables? And what if some of those variables are continuous? We could address the continuous variable problem by discretising it in bins, but we could still end up with a lot of strata, and some of them could have no observations in either the treated or control group. If that is the case, the `subclas_estimator` will become undefined (return `NA`) because the `sdo` cannot be calculated for subgroups that don't have both control and treated observatiions.

This requirement (having observations of both groups in each stratum) is known as the **common support assumption**. And it becomes increasingly difficult to meet this assumption as the number of covariates X increases.

![](images/dimensionlity_strikes_back.jpg)

The reason for this is **the curse of dimensionality**, a phenomena that occurs whenever we work with a dataset with a lot of variables (a.k.a. dimensions). If you come from the ML world you may already be familiar with this problem, but for those who don't, I'm going to drop the [Wikipedia definition](https://en.wikipedia.org/wiki/Curse_of_dimensionality):

> when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. In order to obtain a reliable result, the amount of data needed often grows exponentially with the dimensionality

What can we do:

-   estimate only the ATT

-   

## The (Approximate) Matching Estimator

(skip directly to the approximate matching estimator)

(explain that there a lot of different parameters to decide when we do matching... but here we're going to use a specifical


## Differences with Regression

If the CIA is meet in our data, we could just use regression.

```{r}
lm(y ~ d + region, data = customers) %>% 
  summary()
```

## Distance Matching

## Propensity Scores

### Don't treat propensity scores as a machine learning prediction problem

## Double Robust Estimator

## ATE vs ATT vs ATU: How to get each one with matching?
